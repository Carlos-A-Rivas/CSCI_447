{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "#from dataset import dataset\n",
    "#from algorithm import algorithm\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import numpy as np\n",
    "from scipy.stats import wilcoxon\n",
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset:\n",
    "    def __init__(self, data_path, label_location, processed_flag):\n",
    "        if (processed_flag == False):\n",
    "            # Separating the .data file into lines, and shuffling the lines\n",
    "            with open(data_path, 'r') as file:\n",
    "                lines = file.readlines()\n",
    "            random.shuffle(lines)\n",
    "            self.data_lines = lines\n",
    "            #print(f\"Data Lines Array Unprocessed: {self.data_lines}\")\n",
    "\n",
    "            # Deliminate strings into lists\n",
    "            for i in range(len(self.data_lines)):\n",
    "                self.data_lines[i] = self.data_lines[i].strip()\n",
    "                self.data_lines[i] = self.data_lines[i].split(',')\n",
    "\n",
    "            # If the labels are in the first location, move them to the last location in the line\n",
    "            #print(f\"Data Lines Array Deliminated: {self.data_lines}\")\n",
    "            if (label_location == 'first'):\n",
    "                for i in range(len(self.data_lines)):\n",
    "                    first_item = self.data_lines[i].pop(0)\n",
    "                    self.data_lines[i].append(first_item)\n",
    "                #print(f\"Data Lines Array W/ Indices Swapped: {self.data_lines}\")\n",
    "\n",
    "            # Folding the data...\n",
    "            self.line_count = len(self.data_lines)\n",
    "            partition_size = self.line_count//10\n",
    "            partition_remainder = self.line_count%10\n",
    "            self.partitions = [self.data_lines[i * partition_size:(i + 1) * partition_size] for i in range(10)] # Separating the lines list into 10 lists of lines\n",
    "            if (partition_remainder != 0):\n",
    "                for i in range(partition_remainder):\n",
    "                    # MIGHT NEED TO ADD EXTRA INDICE DIMENSION WHEN DATA IS DELIMINATED PRIOR\n",
    "                    self.partitions[:][i].append(self.data_lines[-(i+1)])\n",
    "        else:\n",
    "            #extract processed data\n",
    "            self.partitions = []\n",
    "            with open(data_path, 'r') as file:  #open csv file of processed data\n",
    "                for line in file:\n",
    "                    line = line.strip() #strip each line of any white space or newlines\n",
    "                    examples = line.split(\";\") #Split each line into examples by each semicolon\n",
    "                    attributes = [list(map(str, item.split(\",\"))) for item in examples] #seperate each attribute and remove commas\n",
    "                    self.partitions.append(attributes) # put into 3d array\n",
    "\n",
    "        # General dataset information setters\n",
    "        self.partition_count = 10\n",
    "        #self.total_example_count = self.line_count\n",
    "        self.attribute_count = len(self.partitions[0][0])\n",
    "\n",
    "    def save(self, save_file_name, save_folder):\n",
    "        # Saves the data based on our convention: Each line is a partition, semicolons separate examples, commas separate attributes/labels\n",
    "        folder_path = os.path.expanduser(f\"{save_folder}/processed_data_new\")  \n",
    "        os.makedirs(folder_path, exist_ok=True)\n",
    "        #get/create the path to the folder that the file should be saved to\n",
    "        file_path = os.path.join(folder_path, save_file_name)\n",
    "        #create the file path\n",
    "        with open(f\"{file_path}.csv\", \"w\") as file:\n",
    "            #open a csv file in the desired location\n",
    "            for line in self.partitions:\n",
    "                partition_lines = \";\".join([\",\".join(map(str, sub_array)) for sub_array in line])\n",
    "                #for each partition, join each example by a semi colon and each attribute by a comma\n",
    "                file.write(partition_lines + \"\\n\")\n",
    "                #write each partition into the file with each \n",
    "        #print(f\"CSV file saved to {file_path}\")\n",
    "        return\n",
    "\n",
    "    def imputate(self, voter_bool):\n",
    "        # Replaces question marks in a dataset with a random value between the min/max of an attribute value\n",
    "        # Breast cancer has a range of 1-10 for the attribute that is missing values\n",
    "        # voter_bool indicates whether you are working with the voter dataset, where attributes are strings of 'y' or 'n' instead of numbers\n",
    "        voter_options = ['y', 'n']\n",
    "        \n",
    "\n",
    "\n",
    "        asdf\n",
    "\n",
    "        asdf\n",
    "        for partition in range(len(self.partitions)):\n",
    "            for example in range(len(self.partitions[partition])):\n",
    "                for attribute in range(len(self.partitions[partition][example])):\n",
    "                    # if this statement is entered that means there is a missing piece of attribute data, so imputation needs to occur at this location\n",
    "                    if (self.partitions[partition][example][attribute] == '?'):\n",
    "                        # This will be the imputation method using 'y' or 'n'\n",
    "                        if (voter_bool == True):\n",
    "                            self.partitions[partition][example][attribute] = random.choice(voter_options)\n",
    "                        # This will be the imputation method using range 1-10\n",
    "                        else:\n",
    "                            self.partitions[partition][example][attribute] = str(random.randint(1,10))\n",
    "\n",
    "\n",
    "\n",
    "    def discretize(self, divisions, glass_iris):\n",
    "        # divisions: how much you want the bins to be divided by\n",
    "        # glass_iris: boolean that indicates whether you are working with the glass or iris dataset. True = glass, False = Iris\n",
    "        glass_extrema = [[1.5112, 1.5339],[10.73,17.38],[0,4.49],[0.29,3.5],[69.81,75.41],[0,6.21],[5.43,16.19],[0,3.15],[0,0.51]]\n",
    "        iris_extrema = [[4.3,7.9],[2.0,4.4],[1.0,6.9],[0.1,2.5]]\n",
    "        glass_ranges = []\n",
    "        iris_ranges = []\n",
    "        # Creates the increment level for the bin ranges\n",
    "        for extrema in glass_extrema:\n",
    "            glass_ranges.append(round((extrema[1]-extrema[0])/divisions,6))\n",
    "        for extrema in iris_extrema:\n",
    "            iris_ranges.append(round((extrema[1]-extrema[0])/divisions,2))\n",
    "\n",
    "        \n",
    "        self.bin_range = []\n",
    "        # Takes a continous attribute and \"bins\" the attribute into discrete groups. May need to create a dictionary\n",
    "        for partition in range(len(self.partitions)):\n",
    "            for example in range(len(self.partitions[partition])):\n",
    "                for attribute in range((len(self.partitions[partition][example])-1)):\n",
    "                    # Run glass discretization\n",
    "                    if (glass_iris == True):\n",
    "                        entered = False\n",
    "                        for i in range(divisions):\n",
    "                            min = (glass_extrema[attribute][0] + (glass_ranges[attribute] * i))\n",
    "                            max = (glass_extrema[attribute][0] + (glass_ranges[attribute] * (i+1)))\n",
    "                            if partition == 0 and example == 0 and attribute == 0:\n",
    "                                self.bin_range.append([])\n",
    "                                self.bin_range[i].append(min)\n",
    "                                self.bin_range[i].append(max)\n",
    "                            if ((float(self.partitions[partition][example][attribute]) >= min) and (float(self.partitions[partition][example][attribute]) <= max)):\n",
    "                                self.partitions[partition][example][attribute] = str(i)\n",
    "                                entered = True\n",
    "                        if entered == False:\n",
    "                            self.partitions[partition][example][attribute] = str(int(float(self.partitions[partition][example][attribute])))\n",
    "                    # Run Iris discretization\n",
    "                    else:\n",
    "                        entered = False\n",
    "                        for i in range(divisions):\n",
    "                            if ((float(self.partitions[partition][example][attribute]) >= (iris_extrema[attribute][0] + (iris_ranges[attribute] * i))) and (float(self.partitions[partition][example][attribute]) <= (iris_extrema[attribute][0] + (iris_ranges[attribute] * (i+1))))):\n",
    "                                self.partitions[partition][example][attribute] = str(i)\n",
    "                                entered = True\n",
    "                            '''\n",
    "                            if entered == False:\n",
    "                                self.partitions[partition][example][attribute] = str(int(float(self.partitions[partition][example][attribute])))\n",
    "                            ''' \n",
    "\n",
    "    def add_noise(self):\n",
    "        #selects 10% of the features at random and shuffles the values within each feature, thus introducing noise into the data\n",
    "        num_to_shuffle = max(1, int(round(0.1 * (self.attribute_count- 1)))) #get the numbr of attributes to shuffle based on 10% of features bing shuffled\n",
    "        #print(f\"{self.attribute_count}\")\n",
    "        attribute_noise = random.sample(range(self.attribute_count - 1), num_to_shuffle)# randomly choose an attribute to shuffle\n",
    "        #print(f\"Noise Index: {attribute_noise}\")\n",
    "        #print(f\"No Noise: {self.partitions}\" + \"\\n\")\n",
    "        for attribute_indice in attribute_noise: #loop through each attribute to shuffle in each example and partition\n",
    "            attributes_values = []\n",
    "            for partition in self.partitions:\n",
    "                for example in partition:\n",
    "                    attributes_values.append(example[attribute_indice]) #store attribute values in a list\n",
    "                    \n",
    "            \n",
    "            random.shuffle(attributes_values) #shuffle selected attribute values\n",
    "            \n",
    "            i = 0\n",
    "            for partition in self.partitions:  #loop through each partition and example\n",
    "                    for example in partition:\n",
    "                        example[attribute_indice] = attributes_values[i] #put the attribute value back into the data after being shuffled\n",
    "                        i += 1\n",
    "    \n",
    "    def remove_attribute(self, indice=0):\n",
    "        # Takes in an attribute indice, and removes that entire indice from the dataset. This can be used to remove ID numbers\n",
    "        for partition in range(len(self.partitions)):\n",
    "            for example in range(len(self.partitions[partition])):\n",
    "                    self.partitions[partition][example].pop(0)\n",
    "    \n",
    "    def fix_data(self):\n",
    "        #self.partitions[-1].append(self.partitions[0].pop(random.randint(0,len(self.partitions[0]))))\n",
    "        for partition in range(len(self.partitions)):\n",
    "            #print(f\"Partition {partition} length: {len(self.partitions[partition])}\")\n",
    "            #print(self.partitions[partition])\n",
    "            for i, example in enumerate(self.partitions[partition]):\n",
    "                    if len(example) <= 2:\n",
    "                        self.partitions[partition].pop(i)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
