{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset:\n",
    "    def __init__(self, data_path: str, processed_flag: str):\n",
    "        '''\n",
    "        - INSTANTIATE ALL self VARIABLES IN THE INIT\n",
    "        - take in the .data file, process it where we get a numpy array of strings where dimensions are as follows: self.intake_data[example][features]\n",
    "        - MAKE SURE TO ADD EXTRACT FUNCTIONALITY FOR BOTH THE TUNING SET AND VALIDATION SET\n",
    "        '''\n",
    "        # FINN ADDS UP HERE\n",
    "        self.intake_data = []\n",
    "        self.tune_set = []\n",
    "        self.validate_set = []\n",
    "        self.ninety_data = []\n",
    "        # CARLOS ADDS DOWN HERE\n",
    "\n",
    "        # Data is being read in from original .DATA file\n",
    "        if (processed_flag == False):\n",
    "            # Separating the .data file into lines, and shuffling the lines\n",
    "            with open(data_path, 'r') as file:\n",
    "                lines = file.readlines()\n",
    "\n",
    "            # Deliminate strings into lists\n",
    "            for i in range(len(lines)):\n",
    "                lines[i] = lines[i].strip()\n",
    "                lines[i] = lines[i].split(',')\n",
    "            \n",
    "            # Make the list into a numpy array\n",
    "            self.intake_data = np.array(lines)\n",
    "\n",
    "        '''\n",
    "        # Data is being extracted from a saved CSV File\n",
    "        else:\n",
    "            #extract_data()\n",
    "        '''\n",
    "\n",
    "    def continuize(self):\n",
    "        '''\n",
    "        This method takes in the indices that need to be continuized. This will look like replacing values that are strings with numbers.\n",
    "        We want to make sure we call this method BEFORE we shuffle so that we do not have to keep track of which number corresponds to which\n",
    "        original value. We can figure this out later\n",
    "        '''\n",
    "        string_to_int = {}\n",
    "        next_int = 0\n",
    "        # This function continuizes a single element so it can be vectorized\n",
    "        def convert_to_num(value):\n",
    "            nonlocal next_int\n",
    "            try:\n",
    "                # Try to convert to float\n",
    "                return float(value)\n",
    "            except ValueError:\n",
    "                # If conversion fails, map the string a number\n",
    "                if value not in string_to_int:\n",
    "                    string_to_int[value] = next_int\n",
    "                    next_int += 1\n",
    "                return string_to_int[value]\n",
    "\n",
    "        # Apply convert_to_num to each element in the array\n",
    "        vectorization = np.vectorize(convert_to_num, otypes=[float])\n",
    "        self.intake_data = vectorization(self.intake_data)\n",
    "        return\n",
    "    def impute(self):\n",
    "        # Replaces question marks in a dataset with a random value between the min/max of an attribute value\n",
    "        # Breast cancer has a range of 1-10 for the attribute that is missing values\n",
    "        for ex_idx in range(len(self.intake_data)):\n",
    "            for att_idx in range(len(self.intake_data[ex_idx])):\n",
    "                # if this statement is entered that means there is a missing piece of attribute data, so imputation needs to occur at this location\n",
    "                if (self.intake_data[ex_idx][att_idx] == '?'):\n",
    "                    # This will be the imputation method using range 1-10\n",
    "                        self.intake_data[ex_idx][att_idx] = str(random.randint(1,10))\n",
    "        return\n",
    "    def shuffle(self):\n",
    "        '''\n",
    "        ONLY CALLED AFTER CONTINUIZING AND IMPUTING\n",
    "        - This method will shuffle the self.intake_data by examples\n",
    "        - Consider adding a flag where this can shuffle higher dimensional array (not explicitly necessary)\n",
    "        '''\n",
    "        np.random.shuffle(self.intake_data)\n",
    "        return\n",
    "    def sort(self, prediction_type_flag):\n",
    "        '''\n",
    "        - Sorts the data by its class/target value. We can assume all labels are the last indice of an example.\n",
    "        - The prediction_type_flag essentially tells us if the last indice can be converted to a float or not. Regression datasets are sorted by value\n",
    "        '''\n",
    "        if prediction_type_flag == \"regression\":\n",
    "            #print('REGRESSION')\n",
    "            sorted_data = self.intake_data[self.intake_data[:, -1].astype(np.float32).argsort()]\n",
    "        else:\n",
    "            #print(\"CLASSIFICATION\")\n",
    "            sorted_data = self.intake_data[self.intake_data[:, -1].argsort()]\n",
    "\n",
    "        self.intake_data = sorted_data\n",
    "        return\n",
    "    def split(self):\n",
    "        '''\n",
    "        Puts the first 10% of the data into its own array (self.tune_set), then the remaining data (self.validate_set) into its own array.\n",
    "        We should end up with two arrays, both are sorted and stratified. The validation still will need to be separated into partitions.\n",
    "        '''\n",
    "        tune_data = []\n",
    "\n",
    "        for i, example in enumerate(self.intake_data):\n",
    "            if(i % 10) == 0:\n",
    "                tune_data.append(example)\n",
    "            else:\n",
    "                self.ninety_data.append(example)\n",
    "\n",
    "        self.tune_set = np.array(tune_data)\n",
    "        self.ninety_data = np.array(self.ninety_data)\n",
    "        \n",
    "        return\n",
    "    def fold(self):\n",
    "        '''\n",
    "        This method folds self.validate_set into stratified partitions\n",
    "        '''\n",
    "        shape = (10, (len(self.ninety_data) // 10) + 1, len(self.ninety_data[0]))\n",
    "        null_string = \"null\"\n",
    "        self.validate_set = np.full(shape, null_string)\n",
    "        fold_counts = np.zeros(10)\n",
    "\n",
    "        for i, example in enumerate(self.ninety_data):\n",
    "            fold_index = i % 10\n",
    "            \n",
    "            example_position = fold_counts[fold_index]  #This finds the next null example\n",
    "            self.validate_set[fold_index, int(example_position)] = example\n",
    "\n",
    "        \n",
    "            fold_counts[fold_index] += 1\n",
    "        return\n",
    "    def shuffle_splits(self):\n",
    "        '''\n",
    "        Shuffles the tune set and validate set after they are complete and stratified\n",
    "        '''\n",
    "        np.random.shuffle(self.tune_set)\n",
    "        for partition_idx, partition in enumerate(self.validate_set):\n",
    "            np.random.shuffle(partition)\n",
    "        return\n",
    "    \n",
    "    def remove_attribute(self, indice=0):\n",
    "        # Takes in an attribute indice, and removes that entire indice from the dataset. This can be used to remove ID numbers\n",
    "        self.intake_data = np.delete(self.intake_data, indice, 1)\n",
    "\n",
    "    \n",
    "    def save(self, filename: str):\n",
    "        \"\"\"\n",
    "        Saves a 2D or 3D numpy array (full of strings) to a CSV file.\n",
    "        \"\"\"\n",
    "        folder_path = os.path.expanduser(f\"~/CSCI_447/Project_2/Datasets/processed_data\")  \n",
    "        os.makedirs(folder_path, exist_ok=True)\n",
    "        #get/create the path to the folder that the file should be saved to\n",
    "        tune_file_path = os.path.join(folder_path, (filename+'_tune_set.csv'))\n",
    "        validate_file_path = os.path.join(folder_path, (filename+'_validate_set.csv'))\n",
    "\n",
    "        shape_info = None\n",
    "        with open(tune_file_path, mode='w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            # Write shape information if it is a 3D array\n",
    "            if shape_info:\n",
    "                writer.writerow([\"shape\"] + list(shape_info))\n",
    "            # Write data\n",
    "            writer.writerows(self.tune_set)\n",
    "\n",
    "        reshaped_array = np.array([[';'.join(row) for row in batch] for batch in self.validate_set])\n",
    "        shape_info = self.validate_set.shape\n",
    "        with open(validate_file_path, mode='w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            # Write shape information if it is a 3D array\n",
    "            if shape_info:\n",
    "                writer.writerow([\"shape\"] + list(shape_info))\n",
    "            # Write data\n",
    "            writer.writerows(reshaped_array)\n",
    "\n",
    "    def extract(self, file_path: str):\n",
    "        \"\"\"\n",
    "        Loads data from a CSV file and converts it back to a numpy array in the original format.\n",
    "        \"\"\"\n",
    "        tune_file_path = file_path+'_tune_set.csv'\n",
    "        validate_file_path = file_path+'_validate_set.csv'\n",
    "\n",
    "        with open(tune_file_path, mode='r') as file:\n",
    "            reader = csv.reader(file)\n",
    "            rows = list(reader)\n",
    "        self.tune_set = np.array(rows, dtype=str)\n",
    "\n",
    "\n",
    "        with open(validate_file_path, mode='r') as file:\n",
    "            reader = csv.reader(file)\n",
    "            rows = list(reader)\n",
    "        shape_info = tuple(map(int, rows[0][1:]))\n",
    "        data = rows[1:]\n",
    "        # Split semicolon-delimited strings back into lists for the third dimension\n",
    "        reconstructed_data = [[cell.split(';') for cell in row] for row in data]\n",
    "        self.validate_set = np.array(reconstructed_data, dtype=str).reshape(shape_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all(user: str, shuffle_split: bool):\n",
    "    abalone_data = dataset('/home/'+user+'/CSCI_447/Project_2/Datasets/abalone.data', False)\n",
    "    cancer_data = dataset('/home/'+user+'/CSCI_447/Project_2/Datasets/breast-cancer-wisconsin.data', False)\n",
    "    fire_data = dataset('/home/'+user+'/CSCI_447/Project_2/Datasets/forestfires.data', False)\n",
    "    glass_data = dataset('/home/'+user+'/CSCI_447/Project_2/Datasets/glass.data', False)\n",
    "    machine_data = dataset('/home/'+user+'/CSCI_447/Project_2/Datasets/machine.data', False)\n",
    "    soybean_data = dataset('/home/'+user+'/CSCI_447/Project_2/Datasets/soybean-small.data', False)\n",
    "\n",
    "    abalone_data.continuize()\n",
    "    abalone_data.shuffle()\n",
    "    abalone_data.sort('regression')\n",
    "    abalone_data.split()\n",
    "    abalone_data.fold()\n",
    "\n",
    "    cancer_data.remove_attribute()\n",
    "    cancer_data.impute()\n",
    "    cancer_data.shuffle()\n",
    "    cancer_data.sort('classification')\n",
    "    cancer_data.split()\n",
    "    cancer_data.fold()\n",
    "\n",
    "    fire_data.continuize()\n",
    "    fire_data.shuffle()\n",
    "    fire_data.sort('regression')\n",
    "    fire_data.split()\n",
    "    fire_data.fold()\n",
    "\n",
    "    glass_data.remove_attribute()\n",
    "    glass_data.shuffle()\n",
    "    glass_data.sort('classification')\n",
    "    glass_data.split()\n",
    "    glass_data.fold()\n",
    "\n",
    "    machine_data.continuize()\n",
    "    machine_data.shuffle()\n",
    "    machine_data.sort('regression')\n",
    "    machine_data.split()\n",
    "    machine_data.fold()\n",
    "\n",
    "    soybean_data.shuffle()\n",
    "    soybean_data.sort('classification')\n",
    "    soybean_data.split()\n",
    "    soybean_data.fold()\n",
    "\n",
    "    if (shuffle_split == True) :\n",
    "        abalone_data.shuffle_splits()\n",
    "        cancer_data.shuffle_splits()\n",
    "        fire_data.shuffle_splits()\n",
    "        glass_data.shuffle_splits()\n",
    "        machine_data.shuffle_splits()\n",
    "        soybean_data.shuffle_splits()\n",
    "\n",
    "    abalone_data.save('abalone')\n",
    "    cancer_data.save('cancer')\n",
    "    fire_data.save('fire')\n",
    "    glass_data.save('glass')\n",
    "    machine_data.save('machine')\n",
    "    soybean_data.save('soybean')\n",
    "\n",
    "    return abalone_data, cancer_data, fire_data, glass_data, machine_data, soybean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class knn:\n",
    "    def __init__(self, data: dataset, prediction_type_flag: str, k_n=1, sigma=1.0):\n",
    "        '''\n",
    "        - Set a variable equal to the tune and validation sets\n",
    "        - instantiate self variables\n",
    "        '''\n",
    "        self.k_n = k_n\n",
    "        self.sigma = sigma\n",
    "        self.tune_set = data.tune_set\n",
    "        self.validate_set = data.validate_set\n",
    "        self.prediction_type = prediction_type_flag\n",
    "        self.predictions = []\n",
    "        self.answers = []\n",
    "        return\n",
    "    def tune(self, epochs=40, k_n_increment=1, sigma_increment=1):\n",
    "        # CONSIDER ADDING INCREMENT PARAMETER, WHERE THE PARAMETER DECIDES HOW MUCH EACH PARAMETER\n",
    "        # IS INCREMENTED PER EPOCH. SELF.K_N AND SELF.SIGMA WOULD NEED TO INITIALLY BE SET TO THE\n",
    "        # INCREMENT, AND IN THE FINAL CALCULATION WHEN CHOOSING THE INDICE THE SELF.K_N/SIGMA WOULD\n",
    "        # NEED TO BE MULTIPLIED BY THE INCREMENT\n",
    "        '''\n",
    "        Use default parameters to predict the tune set using each set of 9 partitions as the model.\n",
    "        Performance should be calculated and averaged across the ENTIRE set of models with the given\n",
    "        hyperparameter. A hyperparameter is incremented, and predictions is re-run. This process\n",
    "        repeats until the desired number of epochs are reached.\n",
    "        '''\n",
    "        k_n_scores = []\n",
    "        sigma_scores = []\n",
    "        self.k_n = k_n_increment\n",
    "        self.sigma = sigma_increment\n",
    "        for i in tqdm(range(epochs), desc=\"Tuning K_n...\"):\n",
    "            self.k_n += k_n_increment\n",
    "            k_n_scores.append(self.classify(True))\n",
    "\n",
    "        if (self.prediction_type == 'regression'):    \n",
    "            for i in tqdm(range(epochs), desc=\"Tuning sigma...\"):\n",
    "                self.sigma += sigma_increment\n",
    "                sigma_scores.append(self.regress(True))\n",
    "\n",
    "        k_n_scores = np.array(k_n_scores)\n",
    "        best_k_n_epochs = np.argmax(k_n_scores, axis=0)\n",
    "        self.k_n = (round(np.mean(best_k_n_epochs)) + 1) * k_n_increment\n",
    "        print(f\"Tuned k_n: {self.k_n}\")\n",
    "        if (self.prediction_type == 'regression'):\n",
    "            sigma_scores = np.array(sigma_scores)\n",
    "            best_sigma_epochs = np.argmax(sigma_scores, axis=0)\n",
    "            self.sigma = (round(np.mean(best_sigma_epochs)) + 1) * sigma_increment\n",
    "            print(f\"Tuned sigma: {self.sigma}\")\n",
    "        return  \n",
    "    def classify(self, tuning_flag=False):\n",
    "        '''\n",
    "        classify holdout set repeat for each fold\n",
    "        '''\n",
    "        predictions = []\n",
    "        answers = []\n",
    "        hold_out_fold = self.tune_set\n",
    "        for fold_idx in tqdm(range(10), leave=False):\n",
    "            if (tuning_flag == False):\n",
    "                hold_out_fold = self.validate_set[fold_idx]\n",
    "            model = np.concatenate([self.validate_set[i] for i in range(10) if i != fold_idx])\n",
    "            #print(model.shape)\n",
    "            #print(hold_out_fold.shape)\n",
    "\n",
    "            for test_point in hold_out_fold:\n",
    "                if (test_point[0] != 'null'):\n",
    "                    true_label = test_point[-1]\n",
    "                    neighbor_indices = self.get_neighbors(model, test_point, self.k_n)\n",
    "                    #print(f\"Neighbor Indices:\\n{neighbor_indices}\")\n",
    "                    neighbor_labels = model[neighbor_indices, -1]\n",
    "                    #print(f\"Neighbor Labels: {neighbor_labels}\")\n",
    "                    label_counts = Counter(neighbor_labels)\n",
    "                    predicted_label = label_counts.most_common(1)[0][0]\n",
    "\n",
    "                    predictions.append(predicted_label)\n",
    "                    answers.append(true_label)\n",
    "\n",
    "        self.predictions = np.array(predictions)\n",
    "        self.answers = np.array(answers)\n",
    "        Loss_values = self.calculate_loss()\n",
    "        #print(f\"Loss Values: {Loss_values}\")\n",
    "        return Loss_values\n",
    "    \n",
    "    \n",
    "    def regress(self, tuning_flag=False):\n",
    "        '''\n",
    "        regress each hold out set repeat for each fold\n",
    "        '''\n",
    "        predictions = []\n",
    "        answers = []\n",
    "        hold_out_fold = self.tune_set\n",
    "        for fold_idx in tqdm(range(10), leave=False):\n",
    "            if (tuning_flag == False):\n",
    "                hold_out_fold = self.validate_set[fold_idx]\n",
    "            model = np.concatenate([self.validate_set[i] for i in range(10) if i != fold_idx])\n",
    "            #print(model.shape)\n",
    "            #print(hold_out_fold.shape)\n",
    "\n",
    "            for test_point in hold_out_fold:\n",
    "                if (test_point[0] != 'null'):\n",
    "                    true_label = test_point[-1]\n",
    "                    neighbor_indices = self.get_neighbors(model, test_point, self.k_n)\n",
    "                    #print(f\"Neighbor Indices:\\n{neighbor_indices}\")\n",
    "                    nearest_neighbors = model[neighbor_indices]\n",
    "                    #print(f\"Nearest Neighbors: {nearest_neighbors}\")\n",
    "                    neighbor_values = nearest_neighbors[:, -1]\n",
    "\n",
    "                    distances = np.array([np.linalg.norm(test_point[:-1].astype(float) - neighbor[:-1].astype(float)) for neighbor in nearest_neighbors])\n",
    "                \n",
    "                    rbf_weights = np.exp(- (distances ** 2) / (2 * self.sigma ** 2))\n",
    "                    #print(f\"Should be equal to last indice of the nearest neighbors: {nearest_neighbors[:, -1]}\")\n",
    "                    weighted_sum = np.sum(rbf_weights * nearest_neighbors[:, -1].astype(float))\n",
    "                    weight_total = np.sum(rbf_weights)\n",
    "\n",
    "                    predicted_value = weighted_sum / weight_total if weight_total != 0 else np.mean(neighbor_values.astype(float))\n",
    "\n",
    "                    predictions.append(predicted_value)\n",
    "                    answers.append(true_label)\n",
    "\n",
    "        self.predictions = np.array(predictions)\n",
    "        self.answers = np.array(answers)\n",
    "        Loss_values = self.calculate_loss()\n",
    "        #print(f\"Loss Values: {Loss_values}\")\n",
    "        return Loss_values\n",
    "    \n",
    "\n",
    "    # NEEDS HEAVY EDITING\n",
    "    def calculate_loss(self):\n",
    "            '''\n",
    "            Classifiction: 0/1 loss, F1 score\n",
    "            Regression: Mean squared error, Mean absolute\n",
    "\n",
    "            '''\n",
    "            loss = []\n",
    "            if(self.prediction_type == \"classification\"):\n",
    "                accuracy = np.mean(self.predictions == self.answers)\n",
    "                loss.append(float(accuracy))\n",
    "\n",
    "                unique_classes = np.unique(self.answers)\n",
    "                f1_scores = []\n",
    "                for cls in unique_classes:\n",
    "                    true_positives = sum((self.predictions == cls) & (self.answers == cls))\n",
    "                    predicted_positives = sum(self.predictions == cls)\n",
    "                    actual_positives = sum(self.answers == cls)\n",
    "\n",
    "                    precision = true_positives / predicted_positives if predicted_positives > 0 else 0\n",
    "                    recall = true_positives / actual_positives if actual_positives > 0 else 0\n",
    "\n",
    "                    if precision + recall > 0:\n",
    "                        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "                    else:\n",
    "                        f1 = 0\n",
    "                    f1_scores.append(f1)\n",
    "\n",
    "                loss.append(float(np.mean(f1_scores)))\n",
    "\n",
    "            else:\n",
    "                mse = np.mean(self.answers.astype(float) - self.predictions.astype(float)) ** 2\n",
    "                loss.append(float(mse))\n",
    "\n",
    "                mae = np.mean(np.abs(self.answers.astype(float) - self.predictions.astype(float)))\n",
    "                loss.append(float(mae))\n",
    "            return loss\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    def euclidean_distance(self, point1: np, point2: np):\n",
    "        # np.linalg.norm calculates the euclidean distances between two points\n",
    "        #print(f\"Point 1 type: {point1.shape}\")\n",
    "        #print(f\"Point 2 type: {point2.shape}\")\n",
    "        return np.linalg.norm(point1 - point2)\n",
    "    def get_neighbors(self, model: np, test_point: np, k_n: int):\n",
    "        '''\n",
    "        - Feed this function a NxN numpy array where the first dimension is num of examples and the second dimension is num of freatures\n",
    "        - The second argument is the reference point\n",
    "        - the third argument is the point that is being referenced for distances\n",
    "        - The method returns the class/regression value of the k_n nearest neighbors\n",
    "        '''\n",
    "        #print(f\"Model shape: {model.shape}\")\n",
    "        distances = np.zeros((model.shape[0]), dtype=float)\n",
    "        #print(f\"Distances Shape: {distances.shape}\")\n",
    "        for i, model_point in enumerate(model):\n",
    "            # calculate euclidean distance\n",
    "            # COULD ALWAYS SWAP THIS FUNCTION CALL FOR THE ONE LINER\n",
    "            if (model_point[0] != \"null\"):\n",
    "                #print(f\"test point: {test_point}\")\n",
    "                #print(f\"model point: {model_point}\")\n",
    "                distances[i] = self.euclidean_distance(test_point[:-1].astype(float), model_point[:-1].astype(float))\n",
    "            else:\n",
    "                distances[i] = 10000000\n",
    "        # np.partitions moves the K_n smallest values in an np array to the front of the array. We then slice the array to get the k_n smallest values\n",
    "        smallest_distances = np.partition(distances, k_n)[:k_n]\n",
    "        #print(f\"Smallest distances: {smallest_distances}\")\n",
    "        neighbor_indices = np.where(np.isin(distances, smallest_distances))[0]\n",
    "        #print(f\"Neighbor Indices:\\n{neighbor_indices}\")\n",
    "        nearest_neighbors = model[neighbor_indices]\n",
    "        #print(type(nearest_neighbors))\n",
    "        # CURRENTLY RETURNS THE INDICES OF THE NEAREST NEIGHBORS\n",
    "        return neighbor_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "abalone_data, cancer_data, fire_data, glass_data, machine_data, soybean_data = process_all('carlthedog3', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "abalone_knn = knn(abalone_data, 'regression')\n",
    "cancer_knn = knn(cancer_data, 'classification')\n",
    "fire_knn = knn(fire_data, 'regression')\n",
    "glass_knn = knn(glass_data, \"classification\")\n",
    "machine_knn = knn(machine_data, 'regression')\n",
    "soybean_knn = knn(soybean_data, \"classification\")\n",
    "#abalone_knn.get_neighbors(abalone_data.validate_set[0], abalone_data.tune_set[0], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tuning K_n...: 100%|██████████| 15/15 [00:37<00:00,  2.50s/it]\n",
      "Tuning K_n...: 100%|██████████| 15/15 [00:03<00:00,  3.78it/s]\n",
      "Tuning K_n...: 100%|██████████| 15/15 [00:00<00:00, 28.07it/s]\n"
     ]
    }
   ],
   "source": [
    "cancer_knn.tune(15)\n",
    "glass_knn.tune(15)\n",
    "soybean_knn.tune(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tuning K_n...: 100%|██████████| 15/15 [00:16<00:00,  1.13s/it]\n",
      "Tuning sigma...: 100%|██████████| 15/15 [00:23<00:00,  1.57s/it]\n",
      "Tuning K_n...: 100%|██████████| 15/15 [00:02<00:00,  5.28it/s]\n",
      "Tuning sigma...: 100%|██████████| 15/15 [00:02<00:00,  5.17it/s]\n"
     ]
    }
   ],
   "source": [
    "#abalone_knn.tune(15)\n",
    "fire_knn.tune(15)\n",
    "machine_knn.tune(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0, 1.0]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(cancer_knn.classify())\n",
    "print(glass_knn.classify())\n",
    "print(soybean_knn.classify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[312.32110644587766, 33.42962095299747]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(abalone_knn.regress())\n",
    "print(fire_knn.regress())\n",
    "print(machine_knn.regress())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
