{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import csv\n",
    "\n",
    "class dataset:\n",
    "    def __init__(self, data_path: str, processed_flag: str):\n",
    "        '''\n",
    "        - INSTANTIATE ALL self VARIABLES IN THE INIT\n",
    "        - take in the .data file, process it where we get a numpy array of strings where dimensions are as follows: self.intake_data[example][features]\n",
    "        - MAKE SURE TO ADD EXTRACT FUNCTIONALITY FOR BOTH THE TUNING SET AND VALIDATION SET\n",
    "        '''\n",
    "        # FINN ADDS UP HERE\n",
    "        self.intake_data = []\n",
    "        self.tune_set = []\n",
    "        self.validate_set = []\n",
    "        self.ninety_data = []\n",
    "        # CARLOS ADDS DOWN HERE\n",
    "\n",
    "        # Data is being read in from original .DATA file\n",
    "        if (processed_flag == False):\n",
    "            # Separating the .data file into lines, and shuffling the lines\n",
    "            with open(data_path, 'r') as file:\n",
    "                lines = file.readlines()\n",
    "\n",
    "            # Deliminate strings into lists\n",
    "            for i in range(len(lines)):\n",
    "                lines[i] = lines[i].strip()\n",
    "                lines[i] = lines[i].split(',')\n",
    "            \n",
    "            # Make the list into a numpy array\n",
    "            self.intake_data = np.array(lines)\n",
    "\n",
    "        '''\n",
    "        # Data is being extracted from a saved CSV File\n",
    "        else:\n",
    "            #extract_data()\n",
    "        '''\n",
    "\n",
    "    def continuize(self):\n",
    "        '''\n",
    "        This method takes in the indices that need to be continuized. This will look like replacing values that are strings with numbers.\n",
    "        We want to make sure we call this method BEFORE we shuffle so that we do not have to keep track of which number corresponds to which\n",
    "        original value. We can figure this out later\n",
    "        '''\n",
    "        string_to_int = {}\n",
    "        next_int = 0\n",
    "        # This function continuizes a single element so it can be vectorized\n",
    "        def convert_to_num(value):\n",
    "            nonlocal next_int\n",
    "            try:\n",
    "                # Try to convert to float\n",
    "                return float(value)\n",
    "            except ValueError:\n",
    "                # If conversion fails, map the string a number\n",
    "                if value not in string_to_int:\n",
    "                    string_to_int[value] = next_int\n",
    "                    next_int += 1\n",
    "                return string_to_int[value]\n",
    "\n",
    "        # Apply convert_to_num to each element in the array\n",
    "        vectorization = np.vectorize(convert_to_num, otypes=[float])\n",
    "        self.intake_data = vectorization(self.intake_data)\n",
    "        return\n",
    "    def impute(self):\n",
    "        # Replaces question marks in a dataset with a random value between the min/max of an attribute value\n",
    "        # Breast cancer has a range of 1-10 for the attribute that is missing values\n",
    "        for ex_idx in range(len(self.intake_data)):\n",
    "            for att_idx in range(len(self.intake_data[ex_idx])):\n",
    "                # if this statement is entered that means there is a missing piece of attribute data, so imputation needs to occur at this location\n",
    "                if (self.intake_data[ex_idx][att_idx] == '?'):\n",
    "                    # This will be the imputation method using range 1-10\n",
    "                        self.intake_data[ex_idx][att_idx] = str(random.randint(1,10))\n",
    "        return\n",
    "    def shuffle(self):\n",
    "        '''\n",
    "        ONLY CALLED AFTER CONTINUIZING AND IMPUTING\n",
    "        - This method will shuffle the self.intake_data by examples\n",
    "        - Consider adding a flag where this can shuffle higher dimensional array (not explicitly necessary)\n",
    "        '''\n",
    "        np.random.shuffle(self.intake_data)\n",
    "        return\n",
    "    def sort(self, prediction_type_flag):\n",
    "        '''\n",
    "        - Sorts the data by its class/target value. We can assume all labels are the last indice of an example.\n",
    "        - The prediction_type_flag essentially tells us if the last indice can be converted to a float or not. Regression datasets are sorted by value\n",
    "        '''\n",
    "        if prediction_type_flag == \"regression\":\n",
    "            #print('REGRESSION')\n",
    "            sorted_data = self.intake_data[self.intake_data[:, -1].astype(np.float32).argsort()]\n",
    "        else:\n",
    "            #print(\"CLASSIFICATION\")\n",
    "            sorted_data = self.intake_data[self.intake_data[:, -1].argsort()]\n",
    "\n",
    "        self.intake_data = sorted_data\n",
    "        return\n",
    "    def split(self):\n",
    "        '''\n",
    "        Puts the first 10% of the data into its own array (self.tune_set), then the remaining data (self.validate_set) into its own array.\n",
    "        We should end up with two arrays, both are sorted and stratified. The validation still will need to be separated into partitions.\n",
    "        '''\n",
    "        tune_data = []\n",
    "\n",
    "        for i, example in enumerate(self.intake_data):\n",
    "            if(i % 10) == 0:\n",
    "                tune_data.append(example)\n",
    "            else:\n",
    "                self.ninety_data.append(example)\n",
    "\n",
    "        self.tune_set = np.array(tune_data)\n",
    "        self.ninety_data = np.array(self.ninety_data)\n",
    "        \n",
    "        return\n",
    "    def fold(self):\n",
    "        '''\n",
    "        This method folds self.validate_set into stratified partitions\n",
    "        '''\n",
    "        shape = (10, (len(self.ninety_data) // 10) + 1, len(self.ninety_data[0]))\n",
    "        null_string = \"null\"\n",
    "        self.validate_set = np.full(shape, null_string)\n",
    "        fold_counts = np.zeros(10)\n",
    "\n",
    "        for i, example in enumerate(self.ninety_data):\n",
    "            fold_index = i % 10\n",
    "            \n",
    "            example_position = fold_counts[fold_index]  #This finds the next null example\n",
    "            self.validate_set[fold_index, int(example_position)] = example\n",
    "\n",
    "        \n",
    "            fold_counts[fold_index] += 1\n",
    "        return\n",
    "    def shuffle_splits(self):\n",
    "        '''\n",
    "        Shuffles the tune set and validate set after they are complete and stratified\n",
    "        '''\n",
    "        np.random.shuffle(self.tune_set)\n",
    "        for partition_idx, partition in enumerate(self.validate_set):\n",
    "            np.random.shuffle(partition)\n",
    "        return\n",
    "    \n",
    "    def remove_attribute(self, indice=0):\n",
    "        # Takes in an attribute indice, and removes that entire indice from the dataset. This can be used to remove ID numbers\n",
    "        self.intake_data = np.delete(self.intake_data, indice, 1)\n",
    "\n",
    "    \n",
    "    def save(self, filename: str):\n",
    "        \"\"\"\n",
    "        Saves a 2D or 3D numpy array (full of strings) to a CSV file.\n",
    "        \"\"\"\n",
    "        folder_path = os.path.expanduser(f\"~/CSCI_447/Project_2/Datasets/processed_data\")  \n",
    "        os.makedirs(folder_path, exist_ok=True)\n",
    "        #get/create the path to the folder that the file should be saved to\n",
    "        tune_file_path = os.path.join(folder_path, (filename+'_tune_set.csv'))\n",
    "        validate_file_path = os.path.join(folder_path, (filename+'_validate_set.csv'))\n",
    "\n",
    "        shape_info = None\n",
    "        with open(tune_file_path, mode='w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            # Write shape information if it is a 3D array\n",
    "            if shape_info:\n",
    "                writer.writerow([\"shape\"] + list(shape_info))\n",
    "            # Write data\n",
    "            writer.writerows(self.tune_set)\n",
    "\n",
    "        reshaped_array = np.array([[';'.join(row) for row in batch] for batch in self.validate_set])\n",
    "        shape_info = self.validate_set.shape\n",
    "        with open(validate_file_path, mode='w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            # Write shape information if it is a 3D array\n",
    "            if shape_info:\n",
    "                writer.writerow([\"shape\"] + list(shape_info))\n",
    "            # Write data\n",
    "            writer.writerows(reshaped_array)\n",
    "\n",
    "    def extract(self, file_path: str):\n",
    "        \"\"\"\n",
    "        Loads data from a CSV file and converts it back to a numpy array in the original format.\n",
    "        \"\"\"\n",
    "        tune_file_path = file_path+'_tune_set.csv'\n",
    "        validate_file_path = file_path+'_validate_set.csv'\n",
    "\n",
    "        with open(tune_file_path, mode='r') as file:\n",
    "            reader = csv.reader(file)\n",
    "            rows = list(reader)\n",
    "        self.tune_set = np.array(rows, dtype=str)\n",
    "\n",
    "\n",
    "        with open(validate_file_path, mode='r') as file:\n",
    "            reader = csv.reader(file)\n",
    "            rows = list(reader)\n",
    "        shape_info = tuple(map(int, rows[0][1:]))\n",
    "        data = rows[1:]\n",
    "        # Split semicolon-delimited strings back into lists for the third dimension\n",
    "        reconstructed_data = [[cell.split(';') for cell in row] for row in data]\n",
    "        self.validate_set = np.array(reconstructed_data, dtype=str).reshape(shape_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "class knn:\n",
    "    def __init__(self, data: dataset, prediction_type_flag: str):\n",
    "        '''\n",
    "        - Set a variable equal to the tune and validation sets\n",
    "        - instantiate self variables\n",
    "        '''\n",
    "        self.tune_set = data.tune_set\n",
    "        self.validate_set = data.validate_set\n",
    "        return\n",
    "    def tune(self, prediction_type_flag: str, epochs=50, k_n=1, sigma=1):\n",
    "        '''\n",
    "        Use default parameters to predict the tune set using each set of 9 partitions as the model.\n",
    "        Performance should be calculated and averaged across the ENTIRE set of models with the given\n",
    "        hyperparameter. A hyperparameter is incremented, and predictions is re-run. This process\n",
    "        repeats until the desired number of epochs are reached.\n",
    "        '''\n",
    "        return\n",
    "    def classify(self):\n",
    "        return\n",
    "    def regress(self):\n",
    "        return\n",
    "    def calculate_loss(self):\n",
    "        return\n",
    "    def get_neighbors(self, model: np, test_point: np, k_n: int):\n",
    "        '''\n",
    "        - Feed this function a NxN numpy array where the first dimension is num of examples and the second dimension is num of freatures\n",
    "        - The second argument is the reference point\n",
    "        - the third argument is the point that is being referenced for distances\n",
    "        - The method returns the class/regression value of the k_n nearest neighbors\n",
    "        '''\n",
    "\n",
    "        def euclidean_distance(point1: np, point2: np):\n",
    "            # np.linalg.norm calculates the euclidean distances between two points\n",
    "            #print(f\"Point 1 type: {type(point1)}\")\n",
    "            #print(f\"Point 2 type: {type(point2)}\")\n",
    "            return np.linalg.norm(point1 - point2)\n",
    "                \n",
    "        \n",
    "        distances = np.zeros((model.shape[0]), dtype=float)\n",
    "        for i, model_point in enumerate(model):\n",
    "            # calculate euclidean distance\n",
    "            # COULD ALWAYS SWAP THIS FUNCTION CALL FOR THE ONE LINER\n",
    "            if (model_point[0] != \"null\"):\n",
    "                distances[i] = euclidean_distance(test_point[:-1], model_point[:-1].astype(np.float128))\n",
    "        # np.partitions moves the K_n smallest values in an np array to the front of the array. We then slice the array to get the k_n smallest values\n",
    "        smallest_distances = np.partition(distances, k_n)[:k_n]\n",
    "        print(smallest_distances)\n",
    "        neighbor_indices = np.where((distances == smallest_distances))\n",
    "        print(f\"Neighbor Indices:\\n{neighbor_indices}\")\n",
    "        # MIGHT NEED TO CONVERT NEIGHBOR_INDICES INTO A TUPLE\n",
    "        nearest_neighbors = model[neighbor_indices][-1]\n",
    "        # Technically this returns the class/regression value of the k_n nearest neighbors\n",
    "        return nearest_neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all(user: str, shuffle_split: bool):\n",
    "    abalone_data = dataset('/home/'+user+'/CSCI_447/Project_2/Datasets/abalone.data', False)\n",
    "    cancer_data = dataset('/home/'+user+'/CSCI_447/Project_2/Datasets/breast-cancer-wisconsin.data', False)\n",
    "    fire_data = dataset('/home/'+user+'/CSCI_447/Project_2/Datasets/forestfires.data', False)\n",
    "    glass_data = dataset('/home/'+user+'/CSCI_447/Project_2/Datasets/glass.data', False)\n",
    "    machine_data = dataset('/home/'+user+'/CSCI_447/Project_2/Datasets/machine.data', False)\n",
    "    soybean_data = dataset('/home/'+user+'/CSCI_447/Project_2/Datasets/soybean-small.data', False)\n",
    "\n",
    "    abalone_data.continuize()\n",
    "    abalone_data.shuffle()\n",
    "    abalone_data.sort('regression')\n",
    "    abalone_data.split()\n",
    "    abalone_data.fold()\n",
    "\n",
    "    cancer_data.remove_attribute()\n",
    "    cancer_data.impute()\n",
    "    cancer_data.shuffle()\n",
    "    cancer_data.sort('classification')\n",
    "    cancer_data.split()\n",
    "    cancer_data.fold()\n",
    "\n",
    "    fire_data.continuize()\n",
    "    fire_data.shuffle()\n",
    "    fire_data.sort('regression')\n",
    "    fire_data.split()\n",
    "    fire_data.fold()\n",
    "\n",
    "    glass_data.remove_attribute()\n",
    "    glass_data.shuffle()\n",
    "    glass_data.sort('classification')\n",
    "    glass_data.split()\n",
    "    glass_data.fold()\n",
    "\n",
    "    machine_data.continuize()\n",
    "    machine_data.shuffle()\n",
    "    machine_data.sort('regression')\n",
    "    machine_data.split()\n",
    "    machine_data.fold()\n",
    "\n",
    "    soybean_data.shuffle()\n",
    "    soybean_data.sort('classification')\n",
    "    soybean_data.split()\n",
    "    soybean_data.fold()\n",
    "\n",
    "    if (shuffle_split == True) :\n",
    "        abalone_data.shuffle_splits()\n",
    "        cancer_data.shuffle_splits()\n",
    "        fire_data.shuffle_splits()\n",
    "        glass_data.shuffle_splits()\n",
    "        machine_data.shuffle_splits()\n",
    "        soybean_data.shuffle_splits()\n",
    "\n",
    "    abalone_data.save('abalone')\n",
    "    cancer_data.save('cancer')\n",
    "    fire_data.save('fire')\n",
    "    glass_data.save('glass')\n",
    "    machine_data.save('machine')\n",
    "    soybean_data.save('soybean')\n",
    "\n",
    "    return abalone_data, cancer_data, fire_data, glass_data, machine_data, soybean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "abalone_data, cancer_data, fire_data, glass_data, machine_data, soybean_data = process_all('carlthedog3', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.02575849 0.02799107]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (376,) (2,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[143], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m abalone_knn \u001b[38;5;241m=\u001b[39m knn(abalone_data, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregression\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mabalone_knn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_neighbors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mabalone_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_set\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mabalone_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtune_set\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[140], line 48\u001b[0m, in \u001b[0;36mknn.get_neighbors\u001b[0;34m(self, model, test_point, k_n)\u001b[0m\n\u001b[1;32m     46\u001b[0m smallest_distances \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mpartition(distances, k_n)[:k_n]\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(smallest_distances)\n\u001b[0;32m---> 48\u001b[0m neighbor_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere((\u001b[43mdistances\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msmallest_distances\u001b[49m))\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNeighbor Indices:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mneighbor_indices\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# MIGHT NEED TO CONVERT NEIGHBOR_INDICES INTO A TUPLE\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (376,) (2,) "
     ]
    }
   ],
   "source": [
    "abalone_knn = knn(abalone_data, 'regression')\n",
    "abalone_knn.get_neighbors(abalone_data.validate_set[0], abalone_data.tune_set[0], 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
