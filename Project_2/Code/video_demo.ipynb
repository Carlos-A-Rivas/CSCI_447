{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset import dataset\n",
    "#from knn import knn\n",
    "from enn import enn\n",
    "from kmeans import kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class knn:\n",
    "    def __init__(self, data: dataset, prediction_type_flag: str, k_n=1, sigma=1.0, suppress_plots=True):\n",
    "        '''\n",
    "        - Set a variable equal to the tune and validation sets\n",
    "        - instantiate self variables\n",
    "        '''\n",
    "        self.suppress_plots = suppress_plots\n",
    "        self.k_n = k_n\n",
    "        self.sigma = sigma\n",
    "        self.tune_set = data.tune_set\n",
    "        self.validate_set = data.validate_set\n",
    "        self.prediction_type = prediction_type_flag\n",
    "        self.predictions = []\n",
    "        self.answers = []\n",
    "        return\n",
    "    def plot_loss(self, metrics: list, parameter: str, increment):\n",
    "        # Extract the number of epochs and loss metrics\n",
    "        metrics = np.array(metrics)\n",
    "        epochs = np.arange(1, metrics.shape[0] + 1) * increment  # Assuming epochs start from 1\n",
    "        loss1 = metrics[:, 0]  # First loss metric\n",
    "        loss2 = metrics[:, 1]  # Second loss metric\n",
    "\n",
    "        # Plotting\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(epochs, loss1, label='Loss Metric 1', marker='o')\n",
    "        plt.plot(epochs, loss2, label='Loss Metric 2', marker='o')\n",
    "\n",
    "        # Adding labels and title\n",
    "        plt.xlabel(f'{parameter} Value')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title(f'Loss Metrics vs. {parameter} value')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "        # Show the plot\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "    def tune(self, epochs=15, k_n_increment=1, sigma_increment=1):\n",
    "        # CONSIDER ADDING INCREMENT PARAMETER, WHERE THE PARAMETER DECIDES HOW MUCH EACH PARAMETER\n",
    "        # IS INCREMENTED PER EPOCH. SELF.K_N AND SELF.SIGMA WOULD NEED TO INITIALLY BE SET TO THE\n",
    "        # INCREMENT, AND IN THE FINAL CALCULATION WHEN CHOOSING THE INDICE THE SELF.K_N/SIGMA WOULD\n",
    "        # NEED TO BE MULTIPLIED BY THE INCREMENT\n",
    "        '''\n",
    "        SIGMA IS PRIMARILY AFFECTING THE MSE, CONSIDER ONLY USING MSE TO DETERMINE SIGMA\n",
    "        '''\n",
    "        '''\n",
    "        Use default parameters to predict the tune set using each set of 9 partitions as the model.\n",
    "        Performance should be calculated and averaged across the ENTIRE set of models with the given\n",
    "        hyperparameter. A hyperparameter is incremented, and predictions is re-run. This process\n",
    "        repeats until the desired number of epochs are reached.\n",
    "        '''\n",
    "        k_n_scores = []\n",
    "        sigma_scores = []\n",
    "        self.k_n = k_n_increment\n",
    "        self.sigma = sigma_increment\n",
    "        for i in tqdm(range(epochs), desc=\"Tuning K_n...\"):\n",
    "            self.k_n += k_n_increment\n",
    "            if (self.prediction_type == 'regression'):\n",
    "                k_n_scores.append(self.regress(True))\n",
    "            else:\n",
    "                k_n_scores.append(self.classify(True))\n",
    "        if (self.suppress_plots == False):\n",
    "            self.plot_loss(k_n_scores, 'K_n', k_n_increment)\n",
    "            \n",
    "\n",
    "        if (self.prediction_type == 'regression'):    \n",
    "            for i in tqdm(range(epochs), desc=\"Tuning sigma...\"):\n",
    "                self.sigma += sigma_increment\n",
    "                sigma_scores.append(self.regress(True))\n",
    "            if (self.suppress_plots == False):\n",
    "                self.plot_loss(sigma_scores, 'Sigma', sigma_increment)\n",
    "\n",
    "        k_n_scores = np.array(k_n_scores)\n",
    "        if (self.prediction_type == 'regression'):\n",
    "            best_k_n_epochs = np.argmin(k_n_scores, axis=0)\n",
    "        else:\n",
    "            best_k_n_epochs = np.argmax(k_n_scores, axis=0)\n",
    "        self.k_n = (round(np.mean(best_k_n_epochs)) + 1) * k_n_increment\n",
    "        print(f\"Tuned k_n: {self.k_n}\")\n",
    "        if (self.prediction_type == 'regression'):\n",
    "            # CURRENTLY IS ONLY USING MSE TO TUNE SIGMA\n",
    "            sigma_scores = np.array(sigma_scores)\n",
    "            best_sigma_epochs = np.argmin(sigma_scores, axis=0)\n",
    "            self.sigma = (round(np.mean(best_sigma_epochs[0] + 1))) * sigma_increment\n",
    "            print(f\"Tuned sigma: {self.sigma}\")\n",
    "        return  \n",
    "    def classify(self, tuning_flag=False, demo=False):\n",
    "        '''\n",
    "        classify holdout set repeat for each fold\n",
    "        '''\n",
    "        Loss_values = np.zeros((10, 2))\n",
    "        predictions = []\n",
    "        answers = []\n",
    "        hold_out_fold = self.tune_set\n",
    "        for fold_idx in tqdm(range(10), leave=False):\n",
    "            if (tuning_flag == False):\n",
    "                hold_out_fold = self.validate_set[fold_idx]\n",
    "            model = np.concatenate([self.validate_set[i] for i in range(10) if i != fold_idx])\n",
    "            #print(model.shape)\n",
    "            #print(hold_out_fold.shape)\n",
    "\n",
    "            for i, test_point in enumerate(hold_out_fold):\n",
    "                if (test_point[0] != 'null'):\n",
    "                    true_label = test_point[-1]\n",
    "                    neighbor_indices = self.get_neighbors(model, test_point, self.k_n)\n",
    "                    #print(f\"Neighbor Indices:\\n{neighbor_indices}\")\n",
    "                    nearest_neighbors = model[neighbor_indices]\n",
    "                    neighbor_labels = model[neighbor_indices, -1]\n",
    "                    #print(f\"Neighbor Labels: {neighbor_labels}\")\n",
    "                    label_counts = Counter(neighbor_labels)\n",
    "                    predicted_label = label_counts.most_common(1)[0][0]\n",
    "                    if (demo and fold_idx == 0 and i == 0):\n",
    "                        print(f\"Point being classified:\\n{test_point}\")\n",
    "                        print(f\"Nearest neighbors:\\n{nearest_neighbors}\")\n",
    "                        print(f\"Predicted class:\\n{predicted_label}\")\n",
    "                    predictions.append(float(predicted_label))\n",
    "                    answers.append(true_label)\n",
    "\n",
    "            self.predictions = np.array(predictions)\n",
    "            #print(self.predictions)\n",
    "            self.predictions = np.rint(self.predictions).astype(int).astype(str)\n",
    "            self.answers = np.array(answers).astype(float)\n",
    "            self.answers = np.rint(self.answers).astype(int).astype(str)\n",
    "            #print(f\"Predictions: {self.predictions}\")\n",
    "            #print(f\"Answers: {self.answers}\")\n",
    "            Loss_values[fold_idx] = self.calculate_loss()\n",
    "            predictions = []\n",
    "            answers = []\n",
    "\n",
    "        if tuning_flag:\n",
    "            average_loss = np.mean(Loss_values, axis=0)\n",
    "            return average_loss  \n",
    "        else:\n",
    "            print(f\"Loss: {Loss_values}\")\n",
    "            return Loss_values\n",
    "        \n",
    "    def RBF(test_point, nearest_neighbors, sigma):\n",
    "        neighbor_values = nearest_neighbors[:, -1]\n",
    "        print(f\"Neighbor Values:\\n{neighbor_values}\")\n",
    "        distances = np.array([np.linalg.norm(test_point[:-1].astype(float) - neighbor[:-1].astype(float)) for neighbor in nearest_neighbors])\n",
    "        print(f\"Distances from the test point to each of the neighbors:\\n{distances}\")\n",
    "        rbf_weights = np.exp(- (distances ** 2) / (2 * sigma ** 2))\n",
    "        print(f\"Weight applied to each point based on its distance from the test point:\\n{rbf_weights}\")\n",
    "        #print(f\"Should be equal to last indice of the nearest neighbors: {nearest_neighbors[:, -1]}\")\n",
    "        weighted_sum = np.sum(rbf_weights * nearest_neighbors[:, -1].astype(float))\n",
    "        print(f\"Respective weight * respective neighbor value, all summed together:\\n{weighted_sum}\")\n",
    "        weight_total = np.sum(rbf_weights)\n",
    "        print(f\"Sum of the RBF weights:\\n{weight_total}\")\n",
    "        predicted_value = weighted_sum / weight_total if weight_total != 0 else np.mean(neighbor_values.astype(float))\n",
    "        print(f\"Final predicted value (weighted sum of neighbor values/RBF weight sum):\\n{predicted_value}\")\n",
    "    \n",
    "    def regress(self, tuning_flag=False, demo=False):\n",
    "        '''\n",
    "        regress each hold out set repeat for each fold\n",
    "        '''\n",
    "        Loss_values = np.zeros((10, 2))  \n",
    "        predictions = []\n",
    "        answers = []\n",
    "        hold_out_fold = self.tune_set\n",
    "        for fold_idx in tqdm(range(10), leave=False):\n",
    "            if (tuning_flag == False):\n",
    "                hold_out_fold = self.validate_set[fold_idx]\n",
    "            model = np.concatenate([self.validate_set[i] for i in range(10) if i != fold_idx])\n",
    "            #print(model.shape)\n",
    "            #print(hold_out_fold.shape)\n",
    "\n",
    "            for i, test_point in enumerate(hold_out_fold):\n",
    "                if (test_point[0] != 'null'):\n",
    "                    true_label = test_point[-1]\n",
    "                    neighbor_indices = self.get_neighbors(model, test_point, self.k_n)\n",
    "                    #print(f\"Neighbor Indices:\\n{neighbor_indices}\")\n",
    "                    nearest_neighbors = model[neighbor_indices]\n",
    "                    #print(f\"Nearest Neighbors: {nearest_neighbors}\")\n",
    "                    neighbor_values = nearest_neighbors[:, -1]\n",
    "\n",
    "                    distances = np.array([np.linalg.norm(test_point[:-1].astype(float) - neighbor[:-1].astype(float)) for neighbor in nearest_neighbors])\n",
    "                \n",
    "                    rbf_weights = np.exp(- (distances ** 2) / (2 * self.sigma ** 2))\n",
    "                    #print(f\"Should be equal to last indice of the nearest neighbors: {nearest_neighbors[:, -1]}\")\n",
    "                    weighted_sum = np.sum(rbf_weights * nearest_neighbors[:, -1].astype(float))\n",
    "                    weight_total = np.sum(rbf_weights)\n",
    "\n",
    "                    predicted_value = weighted_sum / weight_total if weight_total != 0 else np.mean(neighbor_values.astype(float))\n",
    "                    if (demo and fold_idx == 0 and i == 0):\n",
    "                        print(f\"Point being regressed:\\n{test_point}\")\n",
    "                        print(f\"Nearest neighbors:\\n{nearest_neighbors}\")\n",
    "                        print(f\"Predicted value:\\n{predicted_value}\")\n",
    "                    predictions.append(predicted_value)\n",
    "                    answers.append(true_label)\n",
    "\n",
    "            self.predictions = np.array(predictions)\n",
    "            self.answers = np.array(answers)\n",
    "\n",
    "            # Calculate loss for the current fold and store it\n",
    "            Loss_values[fold_idx] = self.calculate_loss()\n",
    "\n",
    "        if tuning_flag:\n",
    "            average_loss = np.mean(Loss_values, axis=0)\n",
    "            return average_loss  \n",
    "        else:\n",
    "            return Loss_values\n",
    "    def calculate_loss(self):\n",
    "            '''\n",
    "            Classifiction: 0/1 loss, F1 score\n",
    "            Regression: Mean squared error, Mean absolute\n",
    "\n",
    "            '''\n",
    "            loss = []\n",
    "            if(self.prediction_type == \"classification\"):\n",
    "                accuracy = np.mean(self.predictions == self.answers)\n",
    "                loss.append(float(accuracy))\n",
    "\n",
    "                unique_classes = np.unique(self.answers)\n",
    "                f1_scores = []\n",
    "                for cls in unique_classes:\n",
    "                    true_positives = sum((self.predictions == cls) & (self.answers == cls))\n",
    "                    predicted_positives = sum(self.predictions == cls)\n",
    "                    actual_positives = sum(self.answers == cls)\n",
    "\n",
    "                    precision = true_positives / predicted_positives if predicted_positives > 0 else 0\n",
    "                    recall = true_positives / actual_positives if actual_positives > 0 else 0\n",
    "\n",
    "                    if precision + recall > 0:\n",
    "                        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "                    else:\n",
    "                        f1 = 0\n",
    "                    f1_scores.append(f1)\n",
    "\n",
    "                loss.append(float(np.mean(f1_scores)))\n",
    "\n",
    "            else:\n",
    "                mse = np.mean(self.answers.astype(float) - self.predictions.astype(float)) ** 2\n",
    "                loss.append(float(mse))\n",
    "\n",
    "                mae = np.mean(np.abs(self.answers.astype(float) - self.predictions.astype(float)))\n",
    "                loss.append(float(mae))\n",
    "            return loss\n",
    "    def euclidean_distance(self, point1: np, point2: np):\n",
    "        # np.linalg.norm calculates the euclidean distances between two points\n",
    "        #print(f\"Point 1 type: {point1.shape}\")\n",
    "        #print(f\"Point 2 type: {point2.shape}\")\n",
    "        return np.linalg.norm(point1 - point2)\n",
    "    def get_neighbors(self, model: np, test_point: np, k_n: int):\n",
    "        '''\n",
    "        - Feed this function a NxN numpy array where the first dimension is num of examples and the second dimension is num of freatures\n",
    "        - The second argument is the reference point\n",
    "        - the third argument is the point that is being referenced for distances\n",
    "        - The method returns the class/regression value of the k_n nearest neighbors\n",
    "        '''\n",
    "        #print(f\"Model shape: {model.shape}\")\n",
    "        distances = np.zeros((model.shape[0]), dtype=float)\n",
    "        #print(f\"Distances Shape: {distances.shape}\")\n",
    "        for i, model_point in enumerate(model):\n",
    "            # calculate euclidean distance\n",
    "            # COULD ALWAYS SWAP THIS FUNCTION CALL FOR THE ONE LINER\n",
    "            if (model_point[0] != \"null\"):\n",
    "                #print(f\"test point: {test_point}\")\n",
    "                #print(f\"model point: {model_point}\")\n",
    "                distances[i] = self.euclidean_distance(test_point[:-1].astype(float), model_point[:-1].astype(float))\n",
    "            else:\n",
    "                distances[i] = 10000000\n",
    "        # np.partitions moves the K_n smallest values in an np array to the front of the array. We then slice the array to get the k_n smallest values\n",
    "        smallest_distances = np.partition(distances, k_n)[:k_n]\n",
    "        #print(f\"Smallest distances: {smallest_distances}\")\n",
    "        neighbor_indices = np.where(np.isin(distances, smallest_distances))[0]\n",
    "        #print(f\"Neighbor Indices:\\n{neighbor_indices}\")\n",
    "        nearest_neighbors = model[neighbor_indices]\n",
    "        #print(type(nearest_neighbors))\n",
    "        # CURRENTLY RETURNS THE INDICES OF THE NEAREST NEIGHBORS\n",
    "        return neighbor_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class enn:\n",
    "    def __init__(self, data: dataset, prediction_type_flag: str, k_n=1, sigma=1.0, epsilon=1, suppress_plots=True):\n",
    "        '''\n",
    "        - Set a variable equal to the tune and validation sets\n",
    "        - instantiate self variables\n",
    "        '''\n",
    "        self.suppress_plots = suppress_plots\n",
    "        self.k_n = k_n\n",
    "        self.sigma = sigma\n",
    "        self.epslion = epsilon\n",
    "        self.tune_set = data.tune_set\n",
    "        self.validate_set = data.validate_set\n",
    "        self.prediction_type = prediction_type_flag\n",
    "        self.predictions = []\n",
    "        self.answers = []\n",
    "        self.reduced_models = []\n",
    "        \n",
    "        return\n",
    "    def plot_loss(self, metrics: list, parameter: str, increment):\n",
    "        # Extract the number of epochs and loss metrics\n",
    "        metrics = np.array(metrics)\n",
    "        epochs = np.arange(1, metrics.shape[0] + 1) * increment  # Assuming epochs start from 1\n",
    "        loss1 = metrics[:, 0]  # First loss metric\n",
    "        loss2 = metrics[:, 1]  # Second loss metric\n",
    "\n",
    "        # Plotting\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(epochs, loss1, label='Loss Metric 1', marker='o')\n",
    "        plt.plot(epochs, loss2, label='Loss Metric 2', marker='o')\n",
    "\n",
    "        # Adding labels and title\n",
    "        plt.xlabel(f'{parameter} Value')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title(f'Loss Metrics vs. {parameter} value')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "        # Show the plot\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "    def tune(self, epochs=15, k_n_increment=1, sigma_increment=1):\n",
    "        '''\n",
    "        for fold_idx, fold in enumerate(self.validate_set):\n",
    "            print(f\"Number of examples in OG data fold {fold_idx+1}: {len(fold)}\")\n",
    "        for fold_idx, fold in enumerate(reduced_models):\n",
    "            print(f\"Number of examples in reduced data fold {fold_idx+1}: {len(fold)}\")\n",
    "        '''\n",
    "        self.reduced_models = self.reduce_dataset(self.validate_set)\n",
    "        k_n_scores = []\n",
    "        sigma_scores = []\n",
    "        self.k_n = k_n_increment\n",
    "        self.sigma = sigma_increment\n",
    "        for i in tqdm(range(epochs), desc=\"Tuning K_n...\"):\n",
    "            self.k_n += k_n_increment\n",
    "            if (self.prediction_type == 'regression'):\n",
    "                k_n_scores.append(self.regress(True))\n",
    "            else:\n",
    "                k_n_scores.append(self.classify(True))\n",
    "        if (self.suppress_plots == False):\n",
    "            self.plot_loss(k_n_scores, 'K_n', k_n_increment)\n",
    "            \n",
    "\n",
    "        if (self.prediction_type == 'regression'):    \n",
    "            for i in tqdm(range(epochs), desc=\"Tuning sigma...\"):\n",
    "                self.sigma += sigma_increment\n",
    "                sigma_scores.append(self.regress(True))\n",
    "            if (self.suppress_plots == False):\n",
    "                self.plot_loss(sigma_scores, 'Sigma', sigma_increment)\n",
    "\n",
    "        k_n_scores = np.array(k_n_scores)\n",
    "        if (self.prediction_type == 'regression'):\n",
    "            best_k_n_epochs = np.argmin(k_n_scores, axis=0)\n",
    "        else:\n",
    "            best_k_n_epochs = np.argmax(k_n_scores, axis=0)\n",
    "        self.k_n = (round(np.mean(best_k_n_epochs)) + 1) * k_n_increment\n",
    "        print(f\"Tuned k_n: {self.k_n}\")\n",
    "        if (self.prediction_type == 'regression'):\n",
    "            # CURRENTLY IS ONLY USING MSE TO TUNE SIGMA\n",
    "            sigma_scores = np.array(sigma_scores)\n",
    "            best_sigma_epochs = np.argmin(sigma_scores, axis=0)\n",
    "            self.sigma = (round(np.mean(best_sigma_epochs[0] + 1))) * sigma_increment\n",
    "            print(f\"Tuned sigma: {self.sigma}\")\n",
    "        return  \n",
    "    def reduce_dataset(self, initial_set: np, epsilon = 0.05):\n",
    "        reduced_models = []\n",
    "        padded_folds = []\n",
    "\n",
    "        for fold_idx in tqdm(range(10), desc=\"reducing dataset...\", leave=False):\n",
    "            removal_indices = []\n",
    "            model = np.concatenate([initial_set[i] for i in range(10) if i != fold_idx])\n",
    "            #print(f\"Fold {fold_idx} Model Shape: {model.shape}\")\n",
    "            #print(hold_out_fold.shape)\n",
    "            for test_point_idx, test_point in enumerate(model):\n",
    "                if (test_point[0] != 'null'):\n",
    "                    # Create a new array excluding the test point\n",
    "                    self_classify_model = np.delete(model, test_point_idx, axis=0)\n",
    "                    true_label = test_point[-1]\n",
    "                    neighbor_indices = self.get_neighbors(self_classify_model, test_point, self.k_n)\n",
    "                    #print(f\"Neighbor Indices:\\n{neighbor_indices}\")\n",
    "                    if (self.prediction_type == \"classification\"):\n",
    "                        neighbor_labels = self_classify_model[neighbor_indices, -1]\n",
    "                        #print(f\"Neighbor Labels: {neighbor_labels}\")\n",
    "                        label_counts = Counter(neighbor_labels)\n",
    "                        predicted_label = label_counts.most_common(1)[0][0]\n",
    "                        if (predicted_label != true_label):\n",
    "                            removal_indices.append(test_point_idx)\n",
    "                    else:\n",
    "                        nearest_neighbors = self_classify_model[neighbor_indices]\n",
    "                        #print(f\"Nearest Neighbors: {nearest_neighbors}\")\n",
    "                        neighbor_values = nearest_neighbors[:, -1]\n",
    "                        distances = np.array([np.linalg.norm(test_point[:-1].astype(float) - neighbor[:-1].astype(float)) for neighbor in nearest_neighbors])\n",
    "                        rbf_weights = np.exp(- (distances ** 2) / (2 * self.sigma ** 2))\n",
    "                        #print(f\"Should be equal to last indice of the nearest neighbors: {nearest_neighbors[:, -1]}\")\n",
    "                        weighted_sum = np.sum(rbf_weights * nearest_neighbors[:, -1].astype(float))\n",
    "                        weight_total = np.sum(rbf_weights)\n",
    "\n",
    "                        predicted_value = weighted_sum / weight_total if weight_total != 0 else np.mean(neighbor_values.astype(float))\n",
    "                        if ((abs(float(predicted_value) - float(true_label)) <= epsilon * float(predicted_value)) == False):\n",
    "                            removal_indices.append(test_point_idx)\n",
    "            #print(f\"Fold {fold_idx+1} Shape: {np.delete(model, removal_indices, axis=0).shape}\")\n",
    "            reduced_models.append(np.delete(model, removal_indices, axis=0))\n",
    "        \n",
    "        #print(reduced_models[0])\n",
    "        max_rows = max(fold.shape[0] for fold in reduced_models)\n",
    "        # Pad each array to have the same number of rows (max_rows)\n",
    "        for fold in reduced_models:\n",
    "            pad_width = max_rows - fold.shape[0]\n",
    "            padded_fold = np.pad(fold, ((0, pad_width), (0, 0)), mode='constant', constant_values='null')\n",
    "            padded_folds.append(padded_fold)\n",
    "        # Stack the padded arrays into a 3D array\n",
    "        padded_reduced_models = np.stack(padded_folds)\n",
    "        print(padded_reduced_models.shape)\n",
    "        return padded_reduced_models\n",
    "        #else: # regression\n",
    "\n",
    "\n",
    "        #return\n",
    "    def classify(self, tuning_flag=False):\n",
    "        '''\n",
    "        classify holdout set repeat for each fold\n",
    "        '''\n",
    "        Loss_values = np.zeros((10, 2))\n",
    "        predictions = []\n",
    "        answers = []\n",
    "        hold_out_fold = self.tune_set\n",
    "        for fold_idx in tqdm(range(10), leave=False):\n",
    "            if (tuning_flag == False):\n",
    "                hold_out_fold = self.validate_set[fold_idx]\n",
    "            model = self.reduced_models[fold_idx]\n",
    "            #print(model.shape)\n",
    "            #print(hold_out_fold.shape)\n",
    "\n",
    "            for test_point in hold_out_fold:\n",
    "                if (test_point[0] != 'null'):\n",
    "                    true_label = test_point[-1]\n",
    "                    neighbor_indices = self.get_neighbors(model, test_point, self.k_n)\n",
    "                    #print(f\"Neighbor Indices:\\n{neighbor_indices}\")\n",
    "                    neighbor_labels = model[neighbor_indices, -1]\n",
    "                    #print(f\"Neighbor Labels: {neighbor_labels}\")\n",
    "                    label_counts = Counter(neighbor_labels)\n",
    "                    predicted_label = label_counts.most_common(1)[0][0]\n",
    "\n",
    "                    predictions.append(float(predicted_label))\n",
    "                    answers.append(true_label)\n",
    "\n",
    "            self.predictions = np.array(predictions)\n",
    "            self.predictions = np.rint(self.predictions).astype(int).astype(str)\n",
    "            self.answers = np.array(answers).astype(float)\n",
    "            self.answers = np.rint(self.answers).astype(int).astype(str)\n",
    "            #print(f\"Predictions: {self.predictions}\")\n",
    "            #print(f\"Answers: {self.answers}\")\n",
    "            Loss_values[fold_idx] = self.calculate_loss()\n",
    "            predictions = []\n",
    "            answers = []\n",
    "\n",
    "        if tuning_flag:\n",
    "            average_loss = np.mean(Loss_values, axis=0)\n",
    "            return average_loss  \n",
    "        else:\n",
    "            print(f\"Loss: {Loss_values}\")\n",
    "            return Loss_values  \n",
    "    def regress(self, tuning_flag=False):\n",
    "        '''\n",
    "        regress each hold out set repeat for each fold\n",
    "        '''\n",
    "        Loss_values = np.zeros((10, 2))  \n",
    "        predictions = []\n",
    "        answers = []\n",
    "        hold_out_fold = self.tune_set\n",
    "        for fold_idx in tqdm(range(10), leave=False):\n",
    "            if (tuning_flag == False):\n",
    "                hold_out_fold = self.validate_set[fold_idx]\n",
    "            model = self.reduced_models[fold_idx]\n",
    "            #print(model.shape)\n",
    "            #print(hold_out_fold.shape)\n",
    "\n",
    "            for test_point in hold_out_fold:\n",
    "                if (test_point[0] != 'null'):\n",
    "                    true_label = test_point[-1]\n",
    "                    neighbor_indices = self.get_neighbors(model, test_point, self.k_n)\n",
    "                    #print(f\"Neighbor Indices:\\n{neighbor_indices}\")\n",
    "                    nearest_neighbors = model[neighbor_indices]\n",
    "                    nearest_neighbors = nearest_neighbors[~np.any(nearest_neighbors == 'null', axis=1)]\n",
    "                    #print(f\"Nearest Neighbors: {nearest_neighbors}\")\n",
    "                    neighbor_values = nearest_neighbors[:, -1]\n",
    "\n",
    "                    distances = np.array([np.linalg.norm(test_point[:-1].astype(float) - neighbor[:-1].astype(float)) for neighbor in nearest_neighbors if neighbor[0] != 'null'])\n",
    "                \n",
    "                    rbf_weights = np.exp(- (distances ** 2) / (2 * self.sigma ** 2))\n",
    "                    #print(f\"Should be equal to last indice of the nearest neighbors: {nearest_neighbors[:, -1]}\")\n",
    "                    weighted_sum = np.sum(rbf_weights * nearest_neighbors[:, -1].astype(float))\n",
    "                    weight_total = np.sum(rbf_weights)\n",
    "\n",
    "                    predicted_value = weighted_sum / weight_total if weight_total != 0 else np.mean(neighbor_values.astype(float))\n",
    "\n",
    "                    predictions.append(predicted_value)\n",
    "                    answers.append(true_label)\n",
    "                    \n",
    "            self.predictions = np.array(predictions)\n",
    "            self.answers = np.array(answers)\n",
    "\n",
    "            # Calculate loss for the current fold and store it\n",
    "            Loss_values[fold_idx] = self.calculate_loss()\n",
    "\n",
    "        if tuning_flag:\n",
    "            average_loss = np.mean(Loss_values, axis=0)\n",
    "            return average_loss  \n",
    "        else:\n",
    "            return Loss_values\n",
    "    def calculate_loss(self):\n",
    "            '''\n",
    "            Classifiction: 0/1 loss, F1 score\n",
    "            Regression: Mean squared error, Mean absolute\n",
    "\n",
    "            '''\n",
    "            loss = []\n",
    "            if(self.prediction_type == \"classification\"):\n",
    "                accuracy = np.mean(self.predictions == self.answers)\n",
    "                loss.append(float(accuracy))\n",
    "\n",
    "                unique_classes = np.unique(self.answers)\n",
    "                f1_scores = []\n",
    "                for cls in unique_classes:\n",
    "                    true_positives = sum((self.predictions == cls) & (self.answers == cls))\n",
    "                    predicted_positives = sum(self.predictions == cls)\n",
    "                    actual_positives = sum(self.answers == cls)\n",
    "\n",
    "                    precision = true_positives / predicted_positives if predicted_positives > 0 else 0\n",
    "                    recall = true_positives / actual_positives if actual_positives > 0 else 0\n",
    "\n",
    "                    if precision + recall > 0:\n",
    "                        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "                    else:\n",
    "                        f1 = 0\n",
    "                    f1_scores.append(f1)\n",
    "\n",
    "                loss.append(float(np.mean(f1_scores)))\n",
    "\n",
    "            else:\n",
    "                mse = np.mean(self.answers.astype(float) - self.predictions.astype(float)) ** 2\n",
    "                loss.append(float(mse))\n",
    "\n",
    "                mae = np.mean(np.abs(self.answers.astype(float) - self.predictions.astype(float)))\n",
    "                loss.append(float(mae))\n",
    "            return loss\n",
    "    def euclidean_distance(self, point1: np, point2: np):\n",
    "        # np.linalg.norm calculates the euclidean distances between two points\n",
    "        #print(f\"Point 1 type: {point1.shape}\")\n",
    "        #print(f\"Point 2 type: {point2.shape}\")\n",
    "        return np.linalg.norm(point1 - point2)\n",
    "    def get_neighbors(self, model: np, test_point: np, k_n: int):\n",
    "        '''\n",
    "        - Feed this function a NxN numpy array where the first dimension is num of examples and the second dimension is num of freatures\n",
    "        - The second argument is the reference point\n",
    "        - the third argument is the point that is being referenced for distances\n",
    "        - The method returns the class/regression value of the k_n nearest neighbors\n",
    "        '''\n",
    "        #print(f\"Model shape: {model.shape}\")\n",
    "        distances = np.zeros((model.shape[0]), dtype=float)\n",
    "        #print(f\"Distances Shape: {distances.shape}\")\n",
    "        for i, model_point in enumerate(model):\n",
    "            # calculate euclidean distance\n",
    "            # COULD ALWAYS SWAP THIS FUNCTION CALL FOR THE ONE LINER\n",
    "            if (model_point[-1] != \"null\"):\n",
    "                #print(f\"test point: {test_point}\")\n",
    "                #print(f\"model point: {model_point}\")\n",
    "                distances[i] = self.euclidean_distance(test_point[:-1].astype(float), model_point[:-1].astype(float))\n",
    "            else:\n",
    "                distances[i] = float('inf')\n",
    "        # np.partitions moves the K_n smallest values in an np array to the front of the array. We then slice the array to get the k_n smallest values\n",
    "        #smallest_distances = np.partition(distances, k_n)[:k_n]\n",
    "        #print(f\"Smallest distances: {smallest_distances}\")\n",
    "        neighbor_indices = np.argsort(distances)[:k_n]\n",
    "        #print(f\"Neighbor Indices:\\n{neighbor_indices}\")\n",
    "        nearest_neighbors = model[neighbor_indices]\n",
    "        #print(type(nearest_neighbors))\n",
    "        # CURRENTLY RETURNS THE INDICES OF THE NEAREST NEIGHBORS\n",
    "        return neighbor_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all(user: str, shuffle_split: bool):\n",
    "    abalone_data = dataset('/home/'+user+'/CSCI_447/Project_2/Datasets/abalone.data', False)\n",
    "    cancer_data = dataset('/home/'+user+'/CSCI_447/Project_2/Datasets/breast-cancer-wisconsin.data', False)\n",
    "    fire_data = dataset('/home/'+user+'/CSCI_447/Project_2/Datasets/forestfires.data', False)\n",
    "    glass_data = dataset('/home/'+user+'/CSCI_447/Project_2/Datasets/glass.data', False)\n",
    "    machine_data = dataset('/home/'+user+'/CSCI_447/Project_2/Datasets/machine.data', False)\n",
    "    soybean_data = dataset('/home/'+user+'/CSCI_447/Project_2/Datasets/soybean-small.data', False)\n",
    "\n",
    "    abalone_data.continuize()\n",
    "    abalone_data.shuffle()\n",
    "    abalone_data.sort('regression')\n",
    "    abalone_data.split()\n",
    "    abalone_data.fold()\n",
    "\n",
    "    cancer_data.remove_attribute()\n",
    "    cancer_data.impute()\n",
    "    cancer_data.shuffle()\n",
    "    cancer_data.sort('classification')\n",
    "    cancer_data.split()\n",
    "    cancer_data.fold()\n",
    "\n",
    "    fire_data.continuize()\n",
    "    fire_data.shuffle()\n",
    "    fire_data.sort('regression')\n",
    "    fire_data.split()\n",
    "    fire_data.fold()\n",
    "\n",
    "    glass_data.continuize()\n",
    "    glass_data.remove_attribute()\n",
    "    glass_data.shuffle()\n",
    "    glass_data.sort('classification')\n",
    "    glass_data.split()\n",
    "    glass_data.fold()\n",
    "\n",
    "    machine_data.continuize()\n",
    "    machine_data.shuffle()\n",
    "    machine_data.sort('regression')\n",
    "    machine_data.split()\n",
    "    machine_data.fold()\n",
    "\n",
    "    soybean_data.continuize()\n",
    "    soybean_data.shuffle()\n",
    "    soybean_data.sort('classification')\n",
    "    soybean_data.split()\n",
    "    soybean_data.fold()\n",
    "\n",
    "    if (shuffle_split == True) :\n",
    "        abalone_data.shuffle_splits()\n",
    "        cancer_data.shuffle_splits()\n",
    "        fire_data.shuffle_splits()\n",
    "        glass_data.shuffle_splits()\n",
    "        machine_data.shuffle_splits()\n",
    "        soybean_data.shuffle_splits()\n",
    "\n",
    "    abalone_data.save('abalone')\n",
    "    cancer_data.save('cancer')\n",
    "    fire_data.save('fire')\n",
    "    glass_data.save('glass')\n",
    "    machine_data.save('machine')\n",
    "    soybean_data.save('soybean')\n",
    "\n",
    "    return abalone_data, cancer_data, fire_data, glass_data, machine_data, soybean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abalone_data, cancer_data, fire_data, glass_data, machine_data, soybean_data = process_all('carlthedog3', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means Clustering Tuning + Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_kmeans = kmeans(cancer_data,'classification')\n",
    "glass_kmeans = kmeans(glass_data,'classification')\n",
    "soybean_kmeans = kmeans(soybean_data,'classification')\n",
    "abalone_kmeans = kmeans(abalone_data, 'regression')\n",
    "fire_kmeans = kmeans(fire_data, 'regression')\n",
    "machine_kmeans = kmeans(machine_data, 'regression')\n",
    "\n",
    "cancer_kmeans.tune()\n",
    "glass_kmeans.tune(k_c_increment=3)\n",
    "soybean_kmeans.tune()\n",
    "abalone_kmeans.tune()\n",
    "fire_kmeans.tune()\n",
    "machine_kmeans.tune()\n",
    "\n",
    "cancer_kmeans_results = cancer_kmeans.classify()\n",
    "glass_kmeans_results = glass_kmeans.classify()\n",
    "soybean_kmeans_results = soybean_kmeans.classify()\n",
    "abalone_kmeans_results = abalone_kmeans.regress()\n",
    "fire_kmeans_results = fire_kmeans.regress()\n",
    "machine_kmeans_results = machine_kmeans.regress()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN Tuning + Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_knn = knn(cancer_data, 'classification')\n",
    "glass_knn = knn(glass_data, \"classification\")\n",
    "soybean_knn = knn(soybean_data, \"classification\")\n",
    "abalone_knn = knn(abalone_data, 'regression')\n",
    "fire_knn = knn(fire_data, 'regression')\n",
    "machine_knn = knn(machine_data, 'regression')\n",
    "\n",
    "cancer_knn.tune(15)\n",
    "glass_knn.tune(15)\n",
    "soybean_knn.tune(10)\n",
    "abalone_knn.tune(5)\n",
    "fire_knn.tune(15)\n",
    "machine_knn.tune(10)\n",
    "\n",
    "cancer_knn_results = cancer_knn.classify()\n",
    "glass_knn_results = glass_knn.classify()\n",
    "soybean_knn_results = soybean_knn.classify()\n",
    "abalone_knn_results = abalone_knn.regress()\n",
    "fire_knn_results = fire_knn.regress()\n",
    "machine_knn_results = machine_knn.regress()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENN Tuning + Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_enn = enn(cancer_data, 'classification', k_n=cancer_knn.k_n, sigma=cancer_knn.sigma)\n",
    "glass_enn = enn(glass_data, \"classification\", k_n=glass_knn.k_n, sigma=glass_knn.sigma)\n",
    "soybean_enn = enn(soybean_data, \"classification\", k_n=soybean_knn.k_n, sigma=soybean_knn.sigma)\n",
    "abalone_enn = enn(abalone_data, 'regression', k_n=abalone_knn.k_n, sigma=abalone_knn.sigma)\n",
    "fire_enn = enn(fire_data, 'regression', k_n=fire_knn.k_n, sigma=fire_knn.sigma)\n",
    "machine_enn = enn(machine_data, 'regression', k_n=machine_knn.k_n, sigma=machine_knn.sigma)\n",
    "\n",
    "cancer_enn.tune(15)\n",
    "glass_enn.tune(10)\n",
    "soybean_enn.tune(10)\n",
    "abalone_enn.tune(5)\n",
    "fire_enn.tune(10)\n",
    "machine_enn.tune(25)\n",
    "\n",
    "cancer_enn_results = cancer_enn.classify()\n",
    "glass_enn_results = glass_enn.classify()\n",
    "soybean_enn_results = soybean_enn.classify()\n",
    "abalone_enn_results = abalone_enn.regress()\n",
    "fire_enn_results = fire_enn.regress()\n",
    "machine_enn_results = machine_enn.regress()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning set, Validation set (Validation set is split into 10 folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Soybean Tuning Set:\\n{soybean_data.tune_set}\")\n",
    "print(f\"Soybean Validation Set:\\n{soybean_data.validate_set}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distance function demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_1 = np.array([1,2,3,4])\n",
    "point_2 = np.array([4,5,6,7])\n",
    "print(f\"Distance function calculation (answer should be 6): {soybean_knn.euclidean_distance(point_1, point_2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RBF kernel function demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_point = np.array([2.5, 3.0, 0.0]) # The zero in this test point is a placeholder\n",
    "nearest_neighbors = np.array([\n",
    "    [2.0, 3.5, 5.0], # Neighbor 1\n",
    "    [3.0, 2.5, 4.0], # Neighbor 2\n",
    "    [2.5, 3.0, 6.0], # Neighbor 3\n",
    "    [3.5, 4.0, 3.0], # Neighbor 4\n",
    "    [2.0, 2.0, 5.5]  # Neighbor 5\n",
    "])\n",
    "soybean_knn.RBF(test_point, nearest_neighbors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN classification demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_knn.classify(demo=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN regression demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fire_knn.regress(demo=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Point being edited out of ENN dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
