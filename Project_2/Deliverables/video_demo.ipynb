{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset import dataset\n",
    "#from knn import knn\n",
    "#from enn import enn\n",
    "#from kmeans import kmeans\n",
    "\n",
    "# All files were developed collaboratively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class knn:\n",
    "    def __init__(self, data: dataset, prediction_type_flag: str, k_n=1, sigma=1.0, suppress_plots=True):\n",
    "        '''\n",
    "        - Set a variable equal to the tune and validation sets\n",
    "        - instantiate self variables\n",
    "        '''\n",
    "        self.suppress_plots = suppress_plots\n",
    "        self.k_n = k_n\n",
    "        self.sigma = sigma\n",
    "        self.tune_set = data.tune_set\n",
    "        self.validate_set = data.validate_set\n",
    "        self.prediction_type = prediction_type_flag\n",
    "        self.predictions = []\n",
    "        self.answers = []\n",
    "        return\n",
    "    def plot_loss(self, metrics: list, parameter: str, increment):\n",
    "        # Extract the number of epochs and loss metrics\n",
    "        metrics = np.array(metrics)\n",
    "        epochs = np.arange(1, metrics.shape[0] + 1) * increment  # Assuming epochs start from 1\n",
    "        loss1 = metrics[:, 0]  # First loss metric\n",
    "        loss2 = metrics[:, 1]  # Second loss metric\n",
    "\n",
    "        # Plotting\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(epochs, loss1, label='Loss Metric 1', marker='o')\n",
    "        plt.plot(epochs, loss2, label='Loss Metric 2', marker='o')\n",
    "\n",
    "        # Adding labels and title\n",
    "        plt.xlabel(f'{parameter} Value')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title(f'Loss Metrics vs. {parameter} value')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "        # Show the plot\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "    def tune(self, epochs=15, k_n_increment=1, sigma_increment=1):\n",
    "        # CONSIDER ADDING INCREMENT PARAMETER, WHERE THE PARAMETER DECIDES HOW MUCH EACH PARAMETER\n",
    "        # IS INCREMENTED PER EPOCH. SELF.K_N AND SELF.SIGMA WOULD NEED TO INITIALLY BE SET TO THE\n",
    "        # INCREMENT, AND IN THE FINAL CALCULATION WHEN CHOOSING THE INDICE THE SELF.K_N/SIGMA WOULD\n",
    "        # NEED TO BE MULTIPLIED BY THE INCREMENT\n",
    "        '''\n",
    "        SIGMA IS PRIMARILY AFFECTING THE MSE, CONSIDER ONLY USING MSE TO DETERMINE SIGMA\n",
    "        '''\n",
    "        '''\n",
    "        Use default parameters to predict the tune set using each set of 9 partitions as the model.\n",
    "        Performance should be calculated and averaged across the ENTIRE set of models with the given\n",
    "        hyperparameter. A hyperparameter is incremented, and predictions is re-run. This process\n",
    "        repeats until the desired number of epochs are reached.\n",
    "        '''\n",
    "        k_n_scores = []\n",
    "        sigma_scores = []\n",
    "        self.k_n = k_n_increment\n",
    "        self.sigma = sigma_increment\n",
    "        for i in tqdm(range(epochs), desc=\"Tuning K_n...\"):\n",
    "            self.k_n += k_n_increment\n",
    "            if (self.prediction_type == 'regression'):\n",
    "                k_n_scores.append(self.regress(True))\n",
    "            else:\n",
    "                k_n_scores.append(self.classify(True))\n",
    "        if (self.suppress_plots == False):\n",
    "            self.plot_loss(k_n_scores, 'K_n', k_n_increment)\n",
    "            \n",
    "\n",
    "        if (self.prediction_type == 'regression'):    \n",
    "            for i in tqdm(range(epochs), desc=\"Tuning sigma...\"):\n",
    "                self.sigma += sigma_increment\n",
    "                sigma_scores.append(self.regress(True))\n",
    "            if (self.suppress_plots == False):\n",
    "                self.plot_loss(sigma_scores, 'Sigma', sigma_increment)\n",
    "\n",
    "        k_n_scores = np.array(k_n_scores)\n",
    "        if (self.prediction_type == 'regression'):\n",
    "            best_k_n_epochs = np.argmin(k_n_scores, axis=0)\n",
    "        else:\n",
    "            best_k_n_epochs = np.argmax(k_n_scores, axis=0)\n",
    "        self.k_n = (round(np.mean(best_k_n_epochs)) + 1) * k_n_increment\n",
    "        print(f\"Tuned k_n: {self.k_n}\")\n",
    "        if (self.prediction_type == 'regression'):\n",
    "            # CURRENTLY IS ONLY USING MSE TO TUNE SIGMA\n",
    "            sigma_scores = np.array(sigma_scores)\n",
    "            best_sigma_epochs = np.argmin(sigma_scores, axis=0)\n",
    "            self.sigma = (round(np.mean(best_sigma_epochs[0] + 1))) * sigma_increment\n",
    "            print(f\"Tuned sigma: {self.sigma}\")\n",
    "    def classify(self, tuning_flag=False, demo=False):\n",
    "        '''\n",
    "        classify holdout set repeat for each fold\n",
    "        '''\n",
    "        Loss_values = np.zeros((10, 2))\n",
    "        predictions = []\n",
    "        answers = []\n",
    "        hold_out_fold = self.tune_set\n",
    "        for fold_idx in tqdm(range(10), leave=False):\n",
    "            if (tuning_flag == False):\n",
    "                hold_out_fold = self.validate_set[fold_idx]\n",
    "            model = np.concatenate([self.validate_set[i] for i in range(10) if i != fold_idx])\n",
    "            #print(model.shape)\n",
    "            #print(hold_out_fold.shape)\n",
    "\n",
    "            for i, test_point in enumerate(hold_out_fold):\n",
    "                if (test_point[0] != 'null'):\n",
    "                    true_label = test_point[-1]\n",
    "                    neighbor_indices = self.get_neighbors(model, test_point, self.k_n)\n",
    "                    #print(f\"Neighbor Indices:\\n{neighbor_indices}\")\n",
    "                    nearest_neighbors = model[neighbor_indices]\n",
    "                    neighbor_labels = model[neighbor_indices, -1]\n",
    "                    #print(f\"Neighbor Labels: {neighbor_labels}\")\n",
    "                    label_counts = Counter(neighbor_labels)\n",
    "                    predicted_label = label_counts.most_common(1)[0][0]\n",
    "                    if (demo and fold_idx == 0 and i == 0):\n",
    "                        print(f\"Point being classified:\\n{test_point}\")\n",
    "                        print(f\"Nearest neighbors:\\n{nearest_neighbors}\")\n",
    "                        print(f\"Predicted class:\\n{predicted_label}\")\n",
    "                    predictions.append(float(predicted_label))\n",
    "                    answers.append(true_label)\n",
    "\n",
    "            self.predictions = np.array(predictions)\n",
    "            #print(self.predictions)\n",
    "            self.predictions = np.rint(self.predictions).astype(int).astype(str)\n",
    "            self.answers = np.array(answers).astype(float)\n",
    "            self.answers = np.rint(self.answers).astype(int).astype(str)\n",
    "            #print(f\"Predictions: {self.predictions}\")\n",
    "            #print(f\"Answers: {self.answers}\")\n",
    "            Loss_values[fold_idx] = self.calculate_loss()\n",
    "            predictions = []\n",
    "            answers = []\n",
    "\n",
    "        if tuning_flag:\n",
    "            average_loss = np.mean(Loss_values, axis=0)\n",
    "            return average_loss  \n",
    "        else:\n",
    "            print(f\"Loss: {Loss_values}\")\n",
    "            return Loss_values\n",
    "        \n",
    "    def RBF(self, test_point, nearest_neighbors, sigma):\n",
    "        neighbor_values = nearest_neighbors[:, -1]\n",
    "        print(f\"Neighbor Values:\\n{neighbor_values}\")\n",
    "        distances = np.array([np.linalg.norm(test_point[:-1].astype(float) - neighbor[:-1].astype(float)) for neighbor in nearest_neighbors])\n",
    "        print(f\"Distances from the test point to each of the neighbors:\\n{distances}\")\n",
    "        rbf_weights = np.exp(- (distances ** 2) / (2 * sigma ** 2))\n",
    "        print(f\"Weight applied to each point based on its distance from the test point:\\n{rbf_weights}\")\n",
    "        #print(f\"Should be equal to last indice of the nearest neighbors: {nearest_neighbors[:, -1]}\")\n",
    "        weighted_sum = np.sum(rbf_weights * nearest_neighbors[:, -1].astype(float))\n",
    "        print(f\"Respective weight * respective neighbor value, all summed together:\\n{weighted_sum}\")\n",
    "        weight_total = np.sum(rbf_weights)\n",
    "        print(f\"Sum of the RBF weights:\\n{weight_total}\")\n",
    "        predicted_value = weighted_sum / weight_total if weight_total != 0 else np.mean(neighbor_values.astype(float))\n",
    "        print(f\"Final predicted value (weighted sum of neighbor values/RBF weight sum):\\n{predicted_value}\")\n",
    "    \n",
    "    def regress(self, tuning_flag=False, demo=False):\n",
    "        '''\n",
    "        regress each hold out set repeat for each fold\n",
    "        '''\n",
    "        Loss_values = np.zeros((10, 2))  \n",
    "        predictions = []\n",
    "        answers = []\n",
    "        hold_out_fold = self.tune_set\n",
    "        for fold_idx in tqdm(range(10), leave=False):\n",
    "            if (tuning_flag == False):\n",
    "                hold_out_fold = self.validate_set[fold_idx]\n",
    "            model = np.concatenate([self.validate_set[i] for i in range(10) if i != fold_idx])\n",
    "            #print(model.shape)\n",
    "            #print(hold_out_fold.shape)\n",
    "\n",
    "            for i, test_point in enumerate(hold_out_fold):\n",
    "                if (test_point[0] != 'null'):\n",
    "                    true_label = test_point[-1]\n",
    "                    neighbor_indices = self.get_neighbors(model, test_point, self.k_n)\n",
    "                    #print(f\"Neighbor Indices:\\n{neighbor_indices}\")\n",
    "                    nearest_neighbors = model[neighbor_indices]\n",
    "                    #print(f\"Nearest Neighbors: {nearest_neighbors}\")\n",
    "                    neighbor_values = nearest_neighbors[:, -1]\n",
    "\n",
    "                    distances = np.array([np.linalg.norm(test_point[:-1].astype(float) - neighbor[:-1].astype(float)) for neighbor in nearest_neighbors])\n",
    "                \n",
    "                    rbf_weights = np.exp(- (distances ** 2) / (2 * self.sigma ** 2))\n",
    "                    #print(f\"Should be equal to last indice of the nearest neighbors: {nearest_neighbors[:, -1]}\")\n",
    "                    weighted_sum = np.sum(rbf_weights * nearest_neighbors[:, -1].astype(float))\n",
    "                    weight_total = np.sum(rbf_weights)\n",
    "\n",
    "                    predicted_value = weighted_sum / weight_total if weight_total != 0 else np.mean(neighbor_values.astype(float))\n",
    "                    if (demo and fold_idx == 0 and i == 0):\n",
    "                        print(f\"Point being regressed:\\n{test_point}\")\n",
    "                        print(f\"Nearest neighbors:\\n{nearest_neighbors}\")\n",
    "                        print(f\"Predicted value:\\n{predicted_value}\")\n",
    "                    predictions.append(predicted_value)\n",
    "                    answers.append(true_label)\n",
    "\n",
    "            self.predictions = np.array(predictions)\n",
    "            self.answers = np.array(answers)\n",
    "\n",
    "            # Calculate loss for the current fold and store it\n",
    "            Loss_values[fold_idx] = self.calculate_loss()\n",
    "\n",
    "        if tuning_flag:\n",
    "            average_loss = np.mean(Loss_values, axis=0)\n",
    "            return average_loss  \n",
    "        else:\n",
    "            return Loss_values\n",
    "    def calculate_loss(self):\n",
    "            '''\n",
    "            Classifiction: 0/1 loss, F1 score\n",
    "            Regression: Mean squared error, Mean absolute\n",
    "\n",
    "            '''\n",
    "            loss = []\n",
    "            if(self.prediction_type == \"classification\"):\n",
    "                accuracy = np.mean(self.predictions == self.answers)\n",
    "                loss.append(float(accuracy))\n",
    "\n",
    "                unique_classes = np.unique(self.answers)\n",
    "                f1_scores = []\n",
    "                for cls in unique_classes:\n",
    "                    true_positives = sum((self.predictions == cls) & (self.answers == cls))\n",
    "                    predicted_positives = sum(self.predictions == cls)\n",
    "                    actual_positives = sum(self.answers == cls)\n",
    "\n",
    "                    precision = true_positives / predicted_positives if predicted_positives > 0 else 0\n",
    "                    recall = true_positives / actual_positives if actual_positives > 0 else 0\n",
    "\n",
    "                    if precision + recall > 0:\n",
    "                        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "                    else:\n",
    "                        f1 = 0\n",
    "                    f1_scores.append(f1)\n",
    "\n",
    "                loss.append(float(np.mean(f1_scores)))\n",
    "\n",
    "            else:\n",
    "                mse = np.mean(self.answers.astype(float) - self.predictions.astype(float)) ** 2\n",
    "                loss.append(float(mse))\n",
    "\n",
    "                mae = np.mean(np.abs(self.answers.astype(float) - self.predictions.astype(float)))\n",
    "                loss.append(float(mae))\n",
    "            return loss\n",
    "    def euclidean_distance(self, point1: np, point2: np):\n",
    "        # np.linalg.norm calculates the euclidean distances between two points\n",
    "        #print(f\"Point 1 type: {point1.shape}\")\n",
    "        #print(f\"Point 2 type: {point2.shape}\")\n",
    "        return np.linalg.norm(point1 - point2)\n",
    "    def get_neighbors(self, model: np, test_point: np, k_n: int):\n",
    "        '''\n",
    "        - Feed this function a NxN numpy array where the first dimension is num of examples and the second dimension is num of freatures\n",
    "        - The second argument is the reference point\n",
    "        - the third argument is the point that is being referenced for distances\n",
    "        - The method returns the class/regression value of the k_n nearest neighbors\n",
    "        '''\n",
    "        #print(f\"Model shape: {model.shape}\")\n",
    "        distances = np.zeros((model.shape[0]), dtype=float)\n",
    "        #print(f\"Distances Shape: {distances.shape}\")\n",
    "        for i, model_point in enumerate(model):\n",
    "            # calculate euclidean distance\n",
    "            # COULD ALWAYS SWAP THIS FUNCTION CALL FOR THE ONE LINER\n",
    "            if (model_point[0] != \"null\"):\n",
    "                #print(f\"test point: {test_point}\")\n",
    "                #print(f\"model point: {model_point}\")\n",
    "                distances[i] = self.euclidean_distance(test_point[:-1].astype(float), model_point[:-1].astype(float))\n",
    "            else:\n",
    "                distances[i] = 10000000\n",
    "        # np.partitions moves the K_n smallest values in an np array to the front of the array. We then slice the array to get the k_n smallest values\n",
    "        smallest_distances = np.partition(distances, k_n)[:k_n]\n",
    "        #print(f\"Smallest distances: {smallest_distances}\")\n",
    "        neighbor_indices = np.where(np.isin(distances, smallest_distances))[0]\n",
    "        #print(f\"Neighbor Indices:\\n{neighbor_indices}\")\n",
    "        nearest_neighbors = model[neighbor_indices]\n",
    "        #print(type(nearest_neighbors))\n",
    "        # CURRENTLY RETURNS THE INDICES OF THE NEAREST NEIGHBORS\n",
    "        return neighbor_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class enn:\n",
    "    def __init__(self, data: dataset, prediction_type_flag: str, k_n=1, sigma=1.0, epsilon=1, suppress_plots=True):\n",
    "        '''\n",
    "        - Set a variable equal to the tune and validation sets\n",
    "        - instantiate self variables\n",
    "        '''\n",
    "        self.suppress_plots = suppress_plots\n",
    "        self.k_n = k_n\n",
    "        self.sigma = sigma\n",
    "        self.epslion = epsilon\n",
    "        self.tune_set = data.tune_set\n",
    "        self.validate_set = data.validate_set\n",
    "        self.prediction_type = prediction_type_flag\n",
    "        self.predictions = []\n",
    "        self.answers = []\n",
    "        self.reduced_models = []\n",
    "    def plot_loss(self, metrics: list, parameter: str, increment):\n",
    "        # Extract the number of epochs and loss metrics\n",
    "        metrics = np.array(metrics)\n",
    "        epochs = np.arange(1, metrics.shape[0] + 1) * increment  # Assuming epochs start from 1\n",
    "        loss1 = metrics[:, 0]  # First loss metric\n",
    "        loss2 = metrics[:, 1]  # Second loss metric\n",
    "\n",
    "        # Plotting\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(epochs, loss1, label='Loss Metric 1', marker='o')\n",
    "        plt.plot(epochs, loss2, label='Loss Metric 2', marker='o')\n",
    "\n",
    "        # Adding labels and title\n",
    "        plt.xlabel(f'{parameter} Value')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title(f'Loss Metrics vs. {parameter} value')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "        # Show the plot\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "    def tune(self, epochs=15, k_n_increment=1, sigma_increment=1, demo=False):\n",
    "        '''\n",
    "        for fold_idx, fold in enumerate(self.validate_set):\n",
    "            print(f\"Number of examples in OG data fold {fold_idx+1}: {len(fold)}\")\n",
    "        for fold_idx, fold in enumerate(reduced_models):\n",
    "            print(f\"Number of examples in reduced data fold {fold_idx+1}: {len(fold)}\")\n",
    "        '''\n",
    "        if (demo):\n",
    "            self.reduced_models = self.reduce_dataset(self.validate_set, demo=True)\n",
    "        else:\n",
    "            self.reduced_models = self.reduce_dataset(self.validate_set, demo=True)\n",
    "        k_n_scores = []\n",
    "        sigma_scores = []\n",
    "        self.k_n = k_n_increment\n",
    "        self.sigma = sigma_increment\n",
    "        for i in tqdm(range(epochs), desc=\"Tuning K_n...\"):\n",
    "            self.k_n += k_n_increment\n",
    "            if (self.prediction_type == 'regression'):\n",
    "                k_n_scores.append(self.regress(True))\n",
    "            else:\n",
    "                k_n_scores.append(self.classify(True))\n",
    "        if (self.suppress_plots == False):\n",
    "            self.plot_loss(k_n_scores, 'K_n', k_n_increment)\n",
    "            \n",
    "\n",
    "        if (self.prediction_type == 'regression'):    \n",
    "            for i in tqdm(range(epochs), desc=\"Tuning sigma...\"):\n",
    "                self.sigma += sigma_increment\n",
    "                sigma_scores.append(self.regress(True))\n",
    "            if (self.suppress_plots == False):\n",
    "                self.plot_loss(sigma_scores, 'Sigma', sigma_increment)\n",
    "\n",
    "        k_n_scores = np.array(k_n_scores)\n",
    "        if (self.prediction_type == 'regression'):\n",
    "            best_k_n_epochs = np.argmin(k_n_scores, axis=0)\n",
    "        else:\n",
    "            best_k_n_epochs = np.argmax(k_n_scores, axis=0)\n",
    "        self.k_n = (round(np.mean(best_k_n_epochs)) + 1) * k_n_increment\n",
    "        print(f\"Tuned k_n: {self.k_n}\")\n",
    "        if (self.prediction_type == 'regression'):\n",
    "            # CURRENTLY IS ONLY USING MSE TO TUNE SIGMA\n",
    "            sigma_scores = np.array(sigma_scores)\n",
    "            best_sigma_epochs = np.argmin(sigma_scores, axis=0)\n",
    "            self.sigma = (round(np.mean(best_sigma_epochs[0] + 1))) * sigma_increment\n",
    "            print(f\"Tuned sigma: {self.sigma}\")\n",
    "    def reduce_dataset(self, initial_set: np, epsilon = 0.05, demo=False):\n",
    "        reduced_models = []\n",
    "        padded_folds = []\n",
    "\n",
    "        for fold_idx in tqdm(range(10), desc=\"reducing dataset...\", leave=False):\n",
    "            removal_indices = []\n",
    "            model = np.concatenate([initial_set[i] for i in range(10) if i != fold_idx])\n",
    "            #print(f\"Fold {fold_idx} Model Shape: {model.shape}\")\n",
    "            #print(hold_out_fold.shape)\n",
    "            for test_point_idx, test_point in enumerate(model):\n",
    "                if (test_point[0] != 'null'):\n",
    "                    # Create a new array excluding the test point\n",
    "                    self_classify_model = np.delete(model, test_point_idx, axis=0)\n",
    "                    true_label = test_point[-1]\n",
    "                    neighbor_indices = self.get_neighbors(self_classify_model, test_point, self.k_n)\n",
    "                    #print(f\"Neighbor Indices:\\n{neighbor_indices}\")\n",
    "                    if (self.prediction_type == \"classification\"):\n",
    "                        neighbor_labels = self_classify_model[neighbor_indices, -1]\n",
    "                        #print(f\"Neighbor Labels: {neighbor_labels}\")\n",
    "                        label_counts = Counter(neighbor_labels)\n",
    "                        predicted_label = label_counts.most_common(1)[0][0]\n",
    "                        if (predicted_label != true_label):\n",
    "                            removal_indices.append(test_point_idx)\n",
    "                    else:\n",
    "                        nearest_neighbors = self_classify_model[neighbor_indices]\n",
    "                        #print(f\"Nearest Neighbors: {nearest_neighbors}\")\n",
    "                        neighbor_values = nearest_neighbors[:, -1]\n",
    "                        distances = np.array([np.linalg.norm(test_point[:-1].astype(float) - neighbor[:-1].astype(float)) for neighbor in nearest_neighbors])\n",
    "                        rbf_weights = np.exp(- (distances ** 2) / (2 * self.sigma ** 2))\n",
    "                        #print(f\"Should be equal to last indice of the nearest neighbors: {nearest_neighbors[:, -1]}\")\n",
    "                        weighted_sum = np.sum(rbf_weights * nearest_neighbors[:, -1].astype(float))\n",
    "                        weight_total = np.sum(rbf_weights)\n",
    "\n",
    "                        predicted_value = weighted_sum / weight_total if weight_total != 0 else np.mean(neighbor_values.astype(float))\n",
    "                        if ((abs(float(predicted_value) - float(true_label)) <= epsilon * float(predicted_value)) == False):\n",
    "                            removal_indices.append(test_point_idx)\n",
    "            #print(f\"Fold {fold_idx+1} Shape: {np.delete(model, removal_indices, axis=0).shape}\")\n",
    "            reduced_models.append(np.delete(model, removal_indices, axis=0))\n",
    "        \n",
    "        #print(reduced_models[0])\n",
    "        max_rows = max(fold.shape[0] for fold in reduced_models)\n",
    "        # Pad each array to have the same number of rows (max_rows)\n",
    "        for fold in reduced_models:\n",
    "            pad_width = max_rows - fold.shape[0]\n",
    "            padded_fold = np.pad(fold, ((0, pad_width), (0, 0)), mode='constant', constant_values='null')\n",
    "            padded_folds.append(padded_fold)\n",
    "        # Stack the padded arrays into a 3D array\n",
    "        padded_reduced_models = np.stack(padded_folds)\n",
    "        print(padded_reduced_models.shape)\n",
    "        if (demo == True):\n",
    "            print(f\"Example removed from original dataset:\\n{model[removal_indices[0]]}\")\n",
    "            print(f\"Does the example exist in the original dataset? - {np.any(np.all(initial_set == model[removal_indices[0]], axis=2))}\")\n",
    "            print(f\"Does the example exist in the reduced dataset? - {np.any(np.all(padded_reduced_models == model[removal_indices[0]], axis=2))}\")\n",
    "        return padded_reduced_models\n",
    "        #else: # regression\n",
    "\n",
    "\n",
    "        #return\n",
    "    def classify(self, tuning_flag=False):\n",
    "        '''\n",
    "        classify holdout set repeat for each fold\n",
    "        '''\n",
    "        Loss_values = np.zeros((10, 2))\n",
    "        predictions = []\n",
    "        answers = []\n",
    "        hold_out_fold = self.tune_set\n",
    "        for fold_idx in tqdm(range(10), leave=False):\n",
    "            if (tuning_flag == False):\n",
    "                hold_out_fold = self.validate_set[fold_idx]\n",
    "            model = self.reduced_models[fold_idx]\n",
    "            #print(model.shape)\n",
    "            #print(hold_out_fold.shape)\n",
    "\n",
    "            for test_point in hold_out_fold:\n",
    "                if (test_point[0] != 'null'):\n",
    "                    true_label = test_point[-1]\n",
    "                    neighbor_indices = self.get_neighbors(model, test_point, self.k_n)\n",
    "                    #print(f\"Neighbor Indices:\\n{neighbor_indices}\")\n",
    "                    neighbor_labels = model[neighbor_indices, -1]\n",
    "                    #print(f\"Neighbor Labels: {neighbor_labels}\")\n",
    "                    label_counts = Counter(neighbor_labels)\n",
    "                    predicted_label = label_counts.most_common(1)[0][0]\n",
    "\n",
    "                    predictions.append(float(predicted_label))\n",
    "                    answers.append(true_label)\n",
    "\n",
    "            self.predictions = np.array(predictions)\n",
    "            self.predictions = np.rint(self.predictions).astype(int).astype(str)\n",
    "            self.answers = np.array(answers).astype(float)\n",
    "            self.answers = np.rint(self.answers).astype(int).astype(str)\n",
    "            #print(f\"Predictions: {self.predictions}\")\n",
    "            #print(f\"Answers: {self.answers}\")\n",
    "            Loss_values[fold_idx] = self.calculate_loss()\n",
    "            predictions = []\n",
    "            answers = []\n",
    "\n",
    "        if tuning_flag:\n",
    "            average_loss = np.mean(Loss_values, axis=0)\n",
    "            return average_loss  \n",
    "        else:\n",
    "            print(f\"Loss: {Loss_values}\")\n",
    "            return Loss_values  \n",
    "    def regress(self, tuning_flag=False):\n",
    "        '''\n",
    "        regress each hold out set repeat for each fold\n",
    "        '''\n",
    "        Loss_values = np.zeros((10, 2))  \n",
    "        predictions = []\n",
    "        answers = []\n",
    "        hold_out_fold = self.tune_set\n",
    "        for fold_idx in tqdm(range(10), leave=False):\n",
    "            if (tuning_flag == False):\n",
    "                hold_out_fold = self.validate_set[fold_idx]\n",
    "            model = self.reduced_models[fold_idx]\n",
    "            #print(model.shape)\n",
    "            #print(hold_out_fold.shape)\n",
    "\n",
    "            for test_point in hold_out_fold:\n",
    "                if (test_point[0] != 'null'):\n",
    "                    true_label = test_point[-1]\n",
    "                    neighbor_indices = self.get_neighbors(model, test_point, self.k_n)\n",
    "                    #print(f\"Neighbor Indices:\\n{neighbor_indices}\")\n",
    "                    nearest_neighbors = model[neighbor_indices]\n",
    "                    nearest_neighbors = nearest_neighbors[~np.any(nearest_neighbors == 'null', axis=1)]\n",
    "                    #print(f\"Nearest Neighbors: {nearest_neighbors}\")\n",
    "                    neighbor_values = nearest_neighbors[:, -1]\n",
    "\n",
    "                    distances = np.array([np.linalg.norm(test_point[:-1].astype(float) - neighbor[:-1].astype(float)) for neighbor in nearest_neighbors if neighbor[0] != 'null'])\n",
    "                \n",
    "                    rbf_weights = np.exp(- (distances ** 2) / (2 * self.sigma ** 2))\n",
    "                    #print(f\"Should be equal to last indice of the nearest neighbors: {nearest_neighbors[:, -1]}\")\n",
    "                    weighted_sum = np.sum(rbf_weights * nearest_neighbors[:, -1].astype(float))\n",
    "                    weight_total = np.sum(rbf_weights)\n",
    "\n",
    "                    predicted_value = weighted_sum / weight_total if weight_total != 0 else np.mean(neighbor_values.astype(float))\n",
    "\n",
    "                    predictions.append(predicted_value)\n",
    "                    answers.append(true_label)\n",
    "                    \n",
    "            self.predictions = np.array(predictions)\n",
    "            self.answers = np.array(answers)\n",
    "\n",
    "            # Calculate loss for the current fold and store it\n",
    "            Loss_values[fold_idx] = self.calculate_loss()\n",
    "\n",
    "        if tuning_flag:\n",
    "            average_loss = np.mean(Loss_values, axis=0)\n",
    "            return average_loss  \n",
    "        else:\n",
    "            return Loss_values\n",
    "    def calculate_loss(self):\n",
    "            '''\n",
    "            Classifiction: 0/1 loss, F1 score\n",
    "            Regression: Mean squared error, Mean absolute\n",
    "\n",
    "            '''\n",
    "            loss = []\n",
    "            if(self.prediction_type == \"classification\"):\n",
    "                accuracy = np.mean(self.predictions == self.answers)\n",
    "                loss.append(float(accuracy))\n",
    "\n",
    "                unique_classes = np.unique(self.answers)\n",
    "                f1_scores = []\n",
    "                for cls in unique_classes:\n",
    "                    true_positives = sum((self.predictions == cls) & (self.answers == cls))\n",
    "                    predicted_positives = sum(self.predictions == cls)\n",
    "                    actual_positives = sum(self.answers == cls)\n",
    "\n",
    "                    precision = true_positives / predicted_positives if predicted_positives > 0 else 0\n",
    "                    recall = true_positives / actual_positives if actual_positives > 0 else 0\n",
    "\n",
    "                    if precision + recall > 0:\n",
    "                        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "                    else:\n",
    "                        f1 = 0\n",
    "                    f1_scores.append(f1)\n",
    "\n",
    "                loss.append(float(np.mean(f1_scores)))\n",
    "\n",
    "            else:\n",
    "                mse = np.mean(self.answers.astype(float) - self.predictions.astype(float)) ** 2\n",
    "                loss.append(float(mse))\n",
    "\n",
    "                mae = np.mean(np.abs(self.answers.astype(float) - self.predictions.astype(float)))\n",
    "                loss.append(float(mae))\n",
    "            return loss\n",
    "    def euclidean_distance(self, point1: np, point2: np):\n",
    "        # np.linalg.norm calculates the euclidean distances between two points\n",
    "        #print(f\"Point 1 type: {point1.shape}\")\n",
    "        #print(f\"Point 2 type: {point2.shape}\")\n",
    "        return np.linalg.norm(point1 - point2)\n",
    "    def get_neighbors(self, model: np, test_point: np, k_n: int):\n",
    "        '''\n",
    "        - Feed this function a NxN numpy array where the first dimension is num of examples and the second dimension is num of freatures\n",
    "        - The second argument is the reference point\n",
    "        - the third argument is the point that is being referenced for distances\n",
    "        - The method returns the class/regression value of the k_n nearest neighbors\n",
    "        '''\n",
    "        #print(f\"Model shape: {model.shape}\")\n",
    "        distances = np.zeros((model.shape[0]), dtype=float)\n",
    "        #print(f\"Distances Shape: {distances.shape}\")\n",
    "        for i, model_point in enumerate(model):\n",
    "            # calculate euclidean distance\n",
    "            # COULD ALWAYS SWAP THIS FUNCTION CALL FOR THE ONE LINER\n",
    "            if (model_point[-1] != \"null\"):\n",
    "                #print(f\"test point: {test_point}\")\n",
    "                #print(f\"model point: {model_point}\")\n",
    "                distances[i] = self.euclidean_distance(test_point[:-1].astype(float), model_point[:-1].astype(float))\n",
    "            else:\n",
    "                distances[i] = float('inf')\n",
    "        # np.partitions moves the K_n smallest values in an np array to the front of the array. We then slice the array to get the k_n smallest values\n",
    "        #smallest_distances = np.partition(distances, k_n)[:k_n]\n",
    "        #print(f\"Smallest distances: {smallest_distances}\")\n",
    "        neighbor_indices = np.argsort(distances)[:k_n]\n",
    "        #print(f\"Neighbor Indices:\\n{neighbor_indices}\")\n",
    "        nearest_neighbors = model[neighbor_indices]\n",
    "        #print(type(nearest_neighbors))\n",
    "        # CURRENTLY RETURNS THE INDICES OF THE NEAREST NEIGHBORS\n",
    "        return neighbor_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class kmeans:\n",
    "    def __init__(self, data: dataset, prediction_type_flag: str, k_c = 1, k_n = 1, sigma = 1.0, suppress_plots=True):\n",
    "        '''\n",
    "        - Set a variable equal to the tune and validation sets\n",
    "        - instantiate self variables\n",
    "        '''\n",
    "        self.suppress_plots = suppress_plots\n",
    "        self.k_n = k_n\n",
    "        self.sigma = sigma\n",
    "        self.k_c = k_c\n",
    "        self.tune_set = data.tune_set\n",
    "        self.validate_set = data.validate_set\n",
    "        self.prediction_type = prediction_type_flag\n",
    "        self.predictions = []\n",
    "        self.answers = []\n",
    "        self.centroids = []\n",
    "    \n",
    "    def plot_loss(self, metrics: list, parameter: str, increment):\n",
    "        # Extract the number of epochs and loss metrics\n",
    "        metrics = np.array(metrics)\n",
    "        epochs = np.arange(1, metrics.shape[0] + 1) * increment  # Assuming epochs start from 1\n",
    "        loss1 = metrics[:, 0]  # First loss metric\n",
    "        loss2 = metrics[:, 1]  # Second loss metric\n",
    "\n",
    "        # Plotting\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(epochs, loss1, label='Loss Metric 1', marker='o')\n",
    "        plt.plot(epochs, loss2, label='Loss Metric 2', marker='o')\n",
    "\n",
    "        # Adding labels and title\n",
    "        plt.xlabel(f'{parameter} Value')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title(f'Loss Metrics vs. {parameter} value')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "        # Show the plot\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "    def tune(self, epochs=15, k_c_increment=1, k_n_increment=1, sigma_increment=1):\n",
    "        '''\n",
    "        Tune number of clusters (k_c), number of neighbors (k_n), and sigma (for regression).\n",
    "        Performance is averaged across all 10 folds. This process repeats for a specified number\n",
    "        of epochs with the hyperparameters incrementing on each epoch.\n",
    "        '''\n",
    "\n",
    "        # Initialize the tuning lists to store performance metrics\n",
    "        k_c_scores = []\n",
    "        k_n_scores = []\n",
    "        sigma_scores = []\n",
    "        \n",
    "        # Initialize hyperparameters\n",
    "        self.k_c = k_c_increment\n",
    "        self.k_n = k_n_increment\n",
    "        self.sigma = sigma_increment\n",
    "\n",
    "        # Tune k_c (number of clusters)\n",
    "        for i in tqdm(range(epochs), desc=\"Tuning K_c...\"):\n",
    "            self.k_c += k_c_increment\n",
    "            self.cluster()  # Re-run clustering with updated k_c\n",
    "            if self.prediction_type == 'regression':\n",
    "                k_c_scores.append(self.regress(True))\n",
    "            else:\n",
    "                k_c_scores.append(self.classify(True))\n",
    "        if (self.suppress_plots == False):\n",
    "            self.plot_loss(k_c_scores, 'K_c', k_c_increment)\n",
    "        \n",
    "        # Tune k_n (number of neighbors)\n",
    "        for i in tqdm(range(epochs), desc=\"Tuning K_n...\"):\n",
    "            self.k_n += k_n_increment\n",
    "            if self.prediction_type == 'regression':\n",
    "                k_n_scores.append(self.regress(True))\n",
    "            else:\n",
    "                k_n_scores.append(self.classify(True))\n",
    "        if (self.suppress_plots == False):\n",
    "            self.plot_loss(k_n_scores, 'K_n', k_n_increment)\n",
    "        \n",
    "        # Tune sigma (only for regression)\n",
    "        if self.prediction_type == 'regression':\n",
    "            for i in tqdm(range(epochs), desc=\"Tuning Sigma...\"):\n",
    "                self.sigma += sigma_increment\n",
    "                sigma_scores.append(self.regress(True))\n",
    "            if (self.suppress_plots == False):\n",
    "                self.plot_loss(sigma_scores, 'Sigma', sigma_increment)\n",
    "\n",
    "        \n",
    "        k_c_scores = np.array(k_c_scores)\n",
    "        if(self.prediction_type == 'classification'):\n",
    "            best_k_c_epochs = np.argmax(k_c_scores, axis=0)\n",
    "        else:\n",
    "            best_k_c_epochs = np.argmin(k_c_scores,axis=0)\n",
    "        self.k_c = (round(np.mean(best_k_c_epochs+1))) * k_c_increment\n",
    "        print(f\"Tuned k_c: {self.k_c}\")\n",
    "\n",
    "        k_n_scores = np.array(k_n_scores)\n",
    "        if(self.prediction_type == 'classification'):\n",
    "            best_k_n_epochs = np.argmax(k_n_scores, axis=0)\n",
    "        else:\n",
    "            best_k_n_epochs = np.argmin(k_n_scores,axis=0)\n",
    "        self.k_n = (round(np.mean(best_k_n_epochs)) + 1) * k_n_increment\n",
    "        print(f\"Tuned k_n: {self.k_n}\")\n",
    "\n",
    "        if self.prediction_type == 'regression':\n",
    "            sigma_scores = np.array(sigma_scores)\n",
    "            best_sigma_epochs = np.argmin(sigma_scores, axis=0)\n",
    "            self.sigma = (round(np.mean(best_sigma_epochs[0] + 1))) * sigma_increment\n",
    "            print(f\"Tuned sigma: {self.sigma}\")\n",
    "\n",
    "   \n",
    "    def cluster(self, demo=False):\n",
    "        \n",
    "        centroids_list = []\n",
    "        \n",
    "        # Get into correct fold \n",
    "        for fold_idx in tqdm(range(10), leave=False): \n",
    "            \n",
    "            model = np.concatenate([self.validate_set[i] for i in range(10) if i != fold_idx])\n",
    "            model[model == 'null'] = np.nan \n",
    "                \n",
    "            model = model.astype(float)\n",
    "\n",
    "            \n",
    "            model = model[~np.isnan(model).any(axis=1)]\n",
    "            \n",
    "            \n",
    "            centroids = model[np.random.choice(model.shape[0], self.k_c, replace=False)]\n",
    "            \n",
    "            prev_centroids = np.copy(centroids)\n",
    "            convergence_threshold = 0.05\n",
    "            max_iterations = 50\n",
    "            iteration = 0\n",
    "\n",
    "            while iteration < max_iterations:\n",
    "                \n",
    "                distances = np.linalg.norm(model[:, np.newaxis] - centroids, axis=2)\n",
    "                labels = np.argmin(distances, axis=1)\n",
    "\n",
    "                if demo and fold_idx == 0:\n",
    "                    print(f\"Data point:\\n{model[0]}\\ndistances to centroids:\\n{distances[0]}\")\n",
    "                    print(f\"Data point {model[0]} assigned to cluster {labels[0]+1}\\n\\n\")\n",
    "\n",
    "                prev_centroids = centroids.copy()\n",
    "\n",
    "                \n",
    "                for i in range(self.k_c):\n",
    "                    if np.any(labels == i): \n",
    "                        centroids[i] = np.nanmean(model[labels == i], axis=0)\n",
    "                    else:\n",
    "                        \n",
    "                        centroids[i] = model[np.random.choice(model.shape[0])]\n",
    "\n",
    "               \n",
    "                relative_change = np.abs(centroids - prev_centroids) / (np.abs(prev_centroids) + 1e-10)\n",
    "                \n",
    "                if np.all(relative_change < convergence_threshold):\n",
    "                    break\n",
    "\n",
    "                iteration += 1\n",
    "\n",
    "            if iteration == max_iterations:\n",
    "                print(\"Warning: Maximum iterations reached without convergence.\")\n",
    "            \n",
    "            \n",
    "            centroids_list.append(centroids)\n",
    "\n",
    "\n",
    "        self.centroids = np.array(centroids_list)\n",
    "        #print(f\"Final Centroids: {self.centroids}\")  # Should be [10 x k_c x feature_count]\n",
    "\n",
    "\n",
    "    \n",
    "    def classify(self, tuning_flag = False, demo=False):\n",
    "        '''\n",
    "        classify holdout set repeat for each fold\n",
    "        '''\n",
    "        if (demo):\n",
    "            self.cluster(demo=True)\n",
    "        else:\n",
    "            self.cluster()\n",
    "        Loss_values = np.zeros((10, 2))\n",
    "        predictions = []\n",
    "        #print(f\"self.k_c = {self.k_c}\")\n",
    "        answers = []\n",
    "        hold_out_fold = self.tune_set\n",
    "        for fold_idx in tqdm(range(10), leave=False):\n",
    "            if (tuning_flag == False):\n",
    "                hold_out_fold = self.validate_set[fold_idx]\n",
    "\n",
    "            model = self.centroids[fold_idx]\n",
    "            #print(model.shape)\n",
    "            #print(hold_out_fold.shape)\n",
    "\n",
    "            for test_point in hold_out_fold:\n",
    "                if (test_point[0] != 'null'):\n",
    "                    true_label = test_point[-1]\n",
    "                    neighbor_indices = self.get_neighbors(model, test_point, self.k_n)\n",
    "                    #print(f\"Neighbor Indices:\\n{neighbor_indices}\")\n",
    "                    neighbor_labels = model[neighbor_indices, -1]\n",
    "                    #print(f\"Neighbor Labels: {neighbor_labels}\")\n",
    "                    label_counts = Counter(neighbor_labels)\n",
    "                    predicted_label = label_counts.most_common(1)[0][0]\n",
    "\n",
    "                    predictions.append(predicted_label)\n",
    "                    answers.append(true_label)\n",
    "\n",
    "            self.predictions = np.array(predictions)\n",
    "            self.predictions = np.rint(self.predictions).astype(int).astype(str)\n",
    "            self.answers = np.array(answers).astype(float)\n",
    "            self.answers = np.rint(self.answers).astype(int).astype(str)\n",
    "        #print(f\"Predictions: {self.predictions}\")\n",
    "        #print(f\"Answers: {self.answers}\")\n",
    "            Loss_values[fold_idx] = self.calculate_loss()\n",
    "            predictions = []\n",
    "            answers = []\n",
    "            #print(f\"Loss Values: {Loss_values}\")\n",
    "        if tuning_flag:\n",
    "            average_loss = np.mean(Loss_values, axis=0)\n",
    "            return average_loss  \n",
    "        else:\n",
    "            print(f\"Loss: {Loss_values}\")\n",
    "            return Loss_values   \n",
    "        \n",
    "    def regress(self, tuning_flag = False):\n",
    "        self.cluster()\n",
    "        predictions = []\n",
    "        answers = []\n",
    "        Loss_values = np.zeros((10, 2))  \n",
    "\n",
    "        hold_out_fold = self.tune_set\n",
    "        for fold_idx in tqdm(range(10), leave=False):\n",
    "            if not tuning_flag:\n",
    "                hold_out_fold = self.validate_set[fold_idx]\n",
    "\n",
    "            model = self.centroids[fold_idx]\n",
    "\n",
    "            for test_point in hold_out_fold:\n",
    "                if test_point[0] != 'null':\n",
    "                    true_label = test_point[-1]\n",
    "                    neighbor_indices = self.get_neighbors(model, test_point, self.k_n)\n",
    "                    nearest_neighbors = model[neighbor_indices]\n",
    "\n",
    "                    distances = np.array([np.linalg.norm(test_point[:-1].astype(float) - neighbor[:-1].astype(float)) for neighbor in nearest_neighbors])\n",
    "                    rbf_weights = np.exp(- (distances ** 2) / (2 * self.sigma ** 2))\n",
    "                    weighted_sum = np.sum(rbf_weights * nearest_neighbors[:, -1].astype(float))\n",
    "                    weight_total = np.sum(rbf_weights)\n",
    "\n",
    "                    predicted_value = weighted_sum / weight_total if weight_total != 0 else np.mean(nearest_neighbors[:, -1].astype(float))\n",
    "\n",
    "                    predictions.append(predicted_value)\n",
    "                    answers.append(true_label)\n",
    "\n",
    "            self.predictions = np.array(predictions)\n",
    "            self.answers = np.array(answers)\n",
    "\n",
    "            # Calculate loss for the current fold and store it\n",
    "            Loss_values[fold_idx] = self.calculate_loss()\n",
    "\n",
    "            \n",
    "            predictions = []\n",
    "            answers = []\n",
    "\n",
    "        if tuning_flag:\n",
    "            average_loss = np.mean(Loss_values, axis=0)\n",
    "            return average_loss  \n",
    "        else:\n",
    "            return Loss_values\n",
    "\n",
    "\n",
    "    def euclidean_distance(self, point1: np, point2: np):\n",
    "        # np.linalg.norm calculates the euclidean distances between two points\n",
    "        #print(f\"Point 1 type: {point1.shape}\")\n",
    "        #print(f\"Point 2 type: {point2.shape}\")\n",
    "        return np.linalg.norm(point1 - point2)\n",
    "    def calculate_loss(self):\n",
    "            '''\n",
    "            Classifiction: 0/1 loss, F1 score\n",
    "            Regression: Mean squared error, Mean absolute\n",
    "\n",
    "            '''\n",
    "            loss = []\n",
    "            if(self.prediction_type == \"classification\"):\n",
    "                #print(self.predictions)\n",
    "                #print(f\"Answers: {self.answers}\")\n",
    "                accuracy = np.mean(self.predictions == self.answers)\n",
    "                loss.append(float(accuracy))\n",
    "\n",
    "                unique_classes = np.unique(self.answers)\n",
    "                f1_scores = []\n",
    "                for cls in unique_classes:\n",
    "                    true_positives = sum((self.predictions == cls) & (self.answers == cls))\n",
    "                    predicted_positives = sum(self.predictions == cls)\n",
    "                    actual_positives = sum(self.answers == cls)\n",
    "\n",
    "                    precision = true_positives / predicted_positives if predicted_positives > 0 else 0\n",
    "                    recall = true_positives / actual_positives if actual_positives > 0 else 0\n",
    "\n",
    "                    if precision + recall > 0:\n",
    "                        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "                    else:\n",
    "                        f1 = 0\n",
    "                    f1_scores.append(f1)\n",
    "\n",
    "                loss.append(float(np.mean(f1_scores)))\n",
    "\n",
    "            else:\n",
    "                mse = np.mean(self.answers.astype(float) - self.predictions.astype(float)) ** 2\n",
    "                loss.append(float(mse))\n",
    "\n",
    "                mae = np.mean(np.abs(self.answers.astype(float) - self.predictions.astype(float)))\n",
    "                loss.append(float(mae))\n",
    "            return loss \n",
    "    def get_neighbors(self, model: np, test_point: np, k_n: int):\n",
    "        '''\n",
    "        - Feed this function a NxN numpy array where the first dimension is num of examples and the second dimension is num of freatures\n",
    "        - The second argument is the reference point\n",
    "        - the third argument is the point that is being referenced for distances\n",
    "        - The method returns the class/regression value of the k_n nearest neighbors\n",
    "        '''\n",
    "        #print(f\"Model shape: {model.shape}\")\n",
    "        \n",
    "\n",
    "        distances = np.zeros(model.shape[0], dtype=float)\n",
    "        #print(f\"Distances Shape: {distances.shape}\")\n",
    "        for i, model_point in enumerate(model):\n",
    "            # calculate euclidean distance\n",
    "            # COULD ALWAYS SWAP THIS FUNCTION CALL FOR THE ONE LINER\n",
    "            if (model_point[-1] != \"null\"):\n",
    "                #print(f\"test point: {test_point}\")\n",
    "                #print(f\"model point: {model_point}\")\n",
    "                distances[i] = self.euclidean_distance(test_point[:-1].astype(float), model_point[:-1].astype(float))\n",
    "            else:\n",
    "                distances[i] = float('inf')\n",
    "        # np.partitions moves the K_n smallest values in an np array to the front of the array. We then slice the array to get the k_n smallest values\n",
    "        #smallest_distances = np.partition(distances, k_n)[:k_n]\n",
    "        #print(f\"Smallest distances: {smallest_distances}\")\n",
    "        neighbor_indices = np.argsort(distances)[:k_n]\n",
    "        #print(f\"Neighbor Indices:\\n{neighbor_indices}\")\n",
    "        #print(type(nearest_neighbors))\n",
    "        # CURRENTLY RETURNS THE INDICES OF THE NEAREST NEIGHBORS\n",
    "        return neighbor_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all(user: str, shuffle_split: bool):\n",
    "    abalone_data = dataset('/home/'+user+'/CSCI_447/Project_2/Datasets/abalone.data', False)\n",
    "    cancer_data = dataset('/home/'+user+'/CSCI_447/Project_2/Datasets/breast-cancer-wisconsin.data', False)\n",
    "    fire_data = dataset('/home/'+user+'/CSCI_447/Project_2/Datasets/forestfires.data', False)\n",
    "    glass_data = dataset('/home/'+user+'/CSCI_447/Project_2/Datasets/glass.data', False)\n",
    "    machine_data = dataset('/home/'+user+'/CSCI_447/Project_2/Datasets/machine.data', False)\n",
    "    soybean_data = dataset('/home/'+user+'/CSCI_447/Project_2/Datasets/soybean-small.data', False)\n",
    "\n",
    "    abalone_data.continuize()\n",
    "    abalone_data.normalize()\n",
    "    abalone_data.shuffle()\n",
    "    abalone_data.sort('regression')\n",
    "    abalone_data.split()\n",
    "    abalone_data.fold()\n",
    "\n",
    "    #cancer_data.continuize()\n",
    "    cancer_data.shuffle()\n",
    "    cancer_data.remove_attribute()\n",
    "    cancer_data.impute()\n",
    "    \n",
    "    cancer_data.sort('classification')\n",
    "    cancer_data.split()\n",
    "    cancer_data.fold()\n",
    "\n",
    "    fire_data.continuize()\n",
    "    fire_data.normalize()\n",
    "    fire_data.shuffle()\n",
    "    fire_data.sort('regression')\n",
    "    fire_data.split()\n",
    "    fire_data.fold()\n",
    "\n",
    "    glass_data.continuize()\n",
    "    glass_data.remove_attribute()\n",
    "    glass_data.shuffle()\n",
    "    glass_data.sort('classification')\n",
    "    glass_data.split()\n",
    "    glass_data.fold()\n",
    "\n",
    "    machine_data.continuize()\n",
    "    machine_data.normalize()\n",
    "    machine_data.shuffle()\n",
    "    machine_data.sort('regression')\n",
    "    machine_data.split()\n",
    "    machine_data.fold()\n",
    "\n",
    "    soybean_data.continuize()\n",
    "    soybean_data.shuffle()\n",
    "    soybean_data.sort('classification')\n",
    "    soybean_data.split()\n",
    "    soybean_data.fold()\n",
    "\n",
    "    if (shuffle_split == True) :\n",
    "        abalone_data.shuffle_splits()\n",
    "        cancer_data.shuffle_splits()\n",
    "        fire_data.shuffle_splits()\n",
    "        glass_data.shuffle_splits()\n",
    "        machine_data.shuffle_splits()\n",
    "        soybean_data.shuffle_splits()\n",
    "\n",
    "    abalone_data.save('abalone')\n",
    "    cancer_data.save('cancer')\n",
    "    fire_data.save('fire')\n",
    "    glass_data.save('glass')\n",
    "    machine_data.save('machine')\n",
    "    soybean_data.save('soybean')\n",
    "\n",
    "    return abalone_data, cancer_data, fire_data, glass_data, machine_data, soybean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "abalone_data, cancer_data, fire_data, glass_data, machine_data, soybean_data = process_all('carlthedog3', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means Clustering Tuning + Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tuning K_c...: 100%|██████████| 15/15 [00:01<00:00,  7.92it/s]\n",
      "Tuning K_n...: 100%|██████████| 15/15 [00:01<00:00,  8.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned k_c: 39\n",
      "Tuned k_n: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tuning K_c...: 100%|██████████| 15/15 [00:01<00:00,  8.59it/s]\n",
      "Tuning K_n...: 100%|██████████| 15/15 [00:01<00:00,  7.53it/s]\n",
      "Tuning Sigma...: 100%|██████████| 15/15 [00:02<00:00,  6.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned k_c: 8\n",
      "Tuned k_n: 2\n",
      "Tuned sigma: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: [[0.6        0.46825397]\n",
      " [0.6        0.4993895 ]\n",
      " [0.68421053 0.57777778]\n",
      " [0.57894737 0.52478632]\n",
      " [0.68421053 0.4043956 ]\n",
      " [0.52631579 0.38290598]\n",
      " [0.68421053 0.5992674 ]\n",
      " [0.68421053 0.6       ]\n",
      " [0.68421053 0.53809524]\n",
      " [0.73684211 0.65836386]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                      \r"
     ]
    }
   ],
   "source": [
    "glass_kmeans = kmeans(glass_data,'classification')\n",
    "fire_kmeans = kmeans(fire_data, 'regression')\n",
    "\n",
    "glass_kmeans.tune(k_c_increment=3)\n",
    "fire_kmeans.tune()\n",
    "\n",
    "glass_kmeans_results = glass_kmeans.classify();\n",
    "fire_kmeans_results = fire_kmeans.regress();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN Tuning + Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tuning K_n...: 100%|██████████| 15/15 [00:02<00:00,  5.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned k_n: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tuning K_n...: 100%|██████████| 15/15 [00:16<00:00,  1.07s/it]\n",
      "Tuning sigma...: 100%|██████████| 15/15 [00:16<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned k_n: 7\n",
      "Tuned sigma: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: [[0.65       0.55677656]\n",
      " [0.6        0.5468254 ]\n",
      " [0.68421053 0.39722222]\n",
      " [0.63157895 0.54761905]\n",
      " [0.68421053 0.61313131]\n",
      " [0.63157895 0.51803752]\n",
      " [0.68421053 0.55835668]\n",
      " [0.73684211 0.6647619 ]\n",
      " [0.84210526 0.87518038]\n",
      " [0.57894737 0.4875    ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    }
   ],
   "source": [
    "glass_knn = knn(glass_data, \"classification\")\n",
    "fire_knn = knn(fire_data, 'regression')\n",
    "\n",
    "glass_knn.tune()\n",
    "fire_knn.tune()\n",
    "\n",
    "glass_knn_results = glass_knn.classify();\n",
    "fire_knn_results = fire_knn.regress();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning set, Validation set (Validation set is split into 10 folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soybean Tuning Set:\n",
      "[[0. 1. 2. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 2. 2. 0. 0. 0. 1. 0. 1. 1. 0. 1.\n",
      "  1. 0. 0. 3. 4. 0. 0. 0. 0. 0. 0. 2.]\n",
      " [6. 0. 2. 1. 0. 1. 1. 1. 0. 0. 1. 1. 0. 2. 2. 0. 0. 0. 1. 1. 3. 1. 1. 1.\n",
      "  0. 0. 0. 0. 4. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [2. 1. 2. 0. 0. 1. 1. 2. 0. 0. 1. 1. 0. 2. 2. 0. 0. 0. 1. 0. 1. 2. 0. 0.\n",
      "  0. 0. 0. 3. 4. 0. 0. 0. 0. 0. 1. 3.]\n",
      " [1. 1. 2. 1. 0. 0. 1. 2. 1. 1. 1. 1. 0. 2. 2. 0. 0. 0. 1. 0. 2. 2. 0. 0.\n",
      "  0. 0. 0. 3. 4. 0. 0. 0. 0. 0. 1. 3.]\n",
      " [3. 0. 0. 1. 0. 1. 2. 1. 0. 0. 1. 1. 0. 2. 2. 0. 0. 0. 1. 0. 0. 3. 0. 0.\n",
      "  0. 2. 1. 0. 4. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "Soybean Validation Set Shape:\n",
      "(10, 5, 36)\n",
      "Soybean Validation Set:\n",
      "[[['0.0' '1.0' '2.0' ... '0.0' '0.0' '2.0']\n",
      "  ['4.0' '0.0' '0.0' ... '0.0' '0.0' '1.0']\n",
      "  ['1.0' '1.0' '2.0' ... '0.0' '1.0' '3.0']\n",
      "  ['2.0' '1.0' '1.0' ... '0.0' '1.0' '3.0']\n",
      "  ['3.0' '0.0' '2.0' ... '0.0' '0.0' '0.0']]\n",
      "\n",
      " [['5.0' '0.0' '2.0' ... '0.0' '0.0' '0.0']\n",
      "  ['3.0' '1.0' '1.0' ... '0.0' '1.0' '3.0']\n",
      "  ['2.0' '1.0' '2.0' ... '0.0' '0.0' '2.0']\n",
      "  ['0.0' '1.0' '2.0' ... '0.0' '1.0' '3.0']\n",
      "  ['6.0' '0.0' '0.0' ... '0.0' '0.0' '1.0']]\n",
      "\n",
      " [['6.0' '0.0' '0.0' ... '0.0' '0.0' '1.0']\n",
      "  ['4.0' '0.0' '2.0' ... '0.0' '0.0' '2.0']\n",
      "  ['null' 'null' 'null' ... 'null' 'null' 'null']\n",
      "  ['0.0' '1.0' '2.0' ... '0.0' '1.0' '3.0']\n",
      "  ['6.0' '0.0' '2.0' ... '0.0' '0.0' '0.0']]\n",
      "\n",
      " ...\n",
      "\n",
      " [['4.0' '0.0' '0.0' ... '0.0' '0.0' '1.0']\n",
      "  ['2.0' '1.0' '2.0' ... '0.0' '1.0' '3.0']\n",
      "  ['3.0' '1.0' '2.0' ... '0.0' '1.0' '3.0']\n",
      "  ['5.0' '0.0' '2.0' ... '0.0' '0.0' '0.0']\n",
      "  ['null' 'null' 'null' ... 'null' 'null' 'null']]\n",
      "\n",
      " [['null' 'null' 'null' ... 'null' 'null' 'null']\n",
      "  ['4.0' '0.0' '2.0' ... '0.0' '0.0' '0.0']\n",
      "  ['2.0' '1.0' '2.0' ... '0.0' '0.0' '2.0']\n",
      "  ['3.0' '1.0' '2.0' ... '0.0' '1.0' '3.0']\n",
      "  ['1.0' '1.0' '2.0' ... '0.0' '1.0' '3.0']]\n",
      "\n",
      " [['5.0' '0.0' '0.0' ... '0.0' '0.0' '1.0']\n",
      "  ['2.0' '1.0' '2.0' ... '0.0' '1.0' '3.0']\n",
      "  ['null' 'null' 'null' ... 'null' 'null' 'null']\n",
      "  ['1.0' '1.0' '2.0' ... '0.0' '1.0' '3.0']\n",
      "  ['3.0' '0.0' '2.0' ... '0.0' '0.0' '2.0']]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Soybean Tuning Set:\\n{soybean_data.tune_set}\")\n",
    "print(f\"Soybean Validation Set Shape:\\n{soybean_data.validate_set.shape}\")\n",
    "print(f\"Soybean Validation Set:\\n{soybean_data.validate_set}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distance function demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance function calculation (answer should be 6): 6.0\n"
     ]
    }
   ],
   "source": [
    "point_1 = np.array([1,2,3,4])\n",
    "point_2 = np.array([4,5,6,7])\n",
    "print(f\"Distance function calculation (answer should be 6): {glass_knn.euclidean_distance(point_1, point_2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RBF kernel function demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neighbor Values:\n",
      "[5.  4.  6.  3.  5.5]\n",
      "Distances from the test point to each of the neighbors:\n",
      "[0.70710678 0.70710678 0.         1.41421356 1.11803399]\n",
      "Weight applied to each point based on its distance from the test point:\n",
      "[0.77880078 0.77880078 1.         0.36787944 0.53526143]\n",
      "Respective weight * respective neighbor value, all summed together:\n",
      "17.056783228011415\n",
      "Sum of the RBF weights:\n",
      "3.4607424358332426\n",
      "Final predicted value (weighted sum of neighbor values/RBF weight sum):\n",
      "4.928648561476854\n"
     ]
    }
   ],
   "source": [
    "test_point = np.array([2.5, 3.0, 0.0]) # The zero in this test point is a placeholder\n",
    "nearest_neighbors = np.array([\n",
    "    [2.0, 3.5, 5.0], # Neighbor 1\n",
    "    [3.0, 2.5, 4.0], # Neighbor 2\n",
    "    [2.5, 3.0, 6.0], # Neighbor 3\n",
    "    [3.5, 4.0, 3.0], # Neighbor 4\n",
    "    [2.0, 2.0, 5.5]  # Neighbor 5\n",
    "])\n",
    "glass_knn.RBF(test_point, nearest_neighbors, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN classification demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [00:00<00:00, 47.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Point being classified:\n",
      "['1.51' '13.3' '3.43' '1.43' '72.2' '0.51' '8.6' '0.0' '0.0' '2.0']\n",
      "Nearest neighbors:\n",
      "[['1.52' '13.4' '3.34' '1.23' '72.3' '0.6' '8.83' '0.0' '0.0' '7.0']\n",
      " ['1.51' '13.5' '3.41' '1.52' '72.0' '0.58' '8.79' '0.0' '0.0' '3.0']]\n",
      "Predicted class:\n",
      "7.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: [[0.65       0.55677656]\n",
      " [0.6        0.5468254 ]\n",
      " [0.68421053 0.39722222]\n",
      " [0.63157895 0.54761905]\n",
      " [0.68421053 0.61313131]\n",
      " [0.63157895 0.51803752]\n",
      " [0.68421053 0.55835668]\n",
      " [0.73684211 0.6647619 ]\n",
      " [0.84210526 0.87518038]\n",
      " [0.57894737 0.4875    ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.65      , 0.55677656],\n",
       "       [0.6       , 0.5468254 ],\n",
       "       [0.68421053, 0.39722222],\n",
       "       [0.63157895, 0.54761905],\n",
       "       [0.68421053, 0.61313131],\n",
       "       [0.63157895, 0.51803752],\n",
       "       [0.68421053, 0.55835668],\n",
       "       [0.73684211, 0.6647619 ],\n",
       "       [0.84210526, 0.87518038],\n",
       "       [0.57894737, 0.4875    ]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glass_knn.classify(demo=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN regression demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:00<00:01,  6.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Point being regressed:\n",
      "['7.0' '5.0' '21.0' '17.0' '92.5' '88.0' '698.' '7.1' '22.8' '40.0' '4.0'\n",
      " '0.0' '0.0']\n",
      "Nearest neighbors:\n",
      "[['2.0' '5.0' '19.0' '26.0' '87.5' '77.0' '694.' '5.0' '22.3' '46.0'\n",
      "  '4.0' '0.0' '0.0']\n",
      " ['3.0' '4.0' '21.0' '14.0' '94.3' '85.1' '692.' '15.9' '19.8' '50.0'\n",
      "  '5.4' '0.0' '0.0']\n",
      " ['4.0' '5.0' '21.0' '17.0' '92.5' '88.0' '698.' '7.1' '20.3' '45.0'\n",
      "  '3.1' '0.0' '0.0']\n",
      " ['4.0' '5.0' '21.0' '14.0' '94.3' '85.1' '692.' '15.9' '17.7' '37.0'\n",
      "  '3.6' '0.0' '0.0']\n",
      " ['5.0' '4.0' '21.0' '14.0' '94.3' '85.1' '692.' '15.9' '20.1' '47.0'\n",
      "  '4.9' '0.0' '0.00']\n",
      " ['3.0' '4.0' '21.0' '18.0' '89.7' '90.0' '704.' '4.8' '22.8' '39.0'\n",
      "  '3.6' '0.0' '0.0']\n",
      " ['7.0' '5.0' '21.0' '17.0' '92.5' '88.0' '698.' '7.1' '17.8' '51.0'\n",
      "  '7.2' '0.0' '0.0']]\n",
      "Predicted value:\n",
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[3.24216358e-03, 7.05520050e-02],\n",
       "       [9.65833573e-04, 4.42470136e-02],\n",
       "       [1.40996016e-03, 9.22302804e-02],\n",
       "       [8.19652662e-04, 7.29918436e-02],\n",
       "       [6.22432394e-04, 6.38273201e-02],\n",
       "       [2.29998895e-04, 6.69965753e-02],\n",
       "       [1.70142298e-04, 5.85838964e-02],\n",
       "       [7.28963675e-05, 5.53878214e-02],\n",
       "       [3.59777281e-05, 5.15026848e-02],\n",
       "       [2.85659155e-06, 5.12306895e-02]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fire_knn.regress(demo=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 132, 10)\n",
      "Example removed from original dataset:\n",
      "['1.51' '13.3' '3.43' '1.43' '72.2' '0.51' '8.6' '0.0' '0.0' '2.0']\n",
      "Does the example exist in the original dataset? - True\n",
      "Does the example exist in the reduced dataset? - False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tuning K_n...: 100%|██████████| 15/15 [00:01<00:00,  7.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned k_n: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "glass_enn = enn(glass_data, \"classification\", k_n=glass_knn.k_n, sigma=glass_knn.sigma)\n",
    "glass_enn.tune()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Point being edited out of ENN dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 107, 13)\n",
      "Example removed from original dataset:\n",
      "['7.0' '5.0' '21.0' '17.0' '92.5' '88.0' '698.' '7.1' '22.8' '40.0' '4.0'\n",
      " '0.0' '0.0']\n",
      "Does the example exist in the original dataset? - True\n",
      "Does the example exist in the reduced dataset? - False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tuning K_n...: 100%|██████████| 15/15 [00:04<00:00,  3.37it/s]\n",
      "Tuning sigma...: 100%|██████████| 15/15 [00:04<00:00,  3.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned k_n: 2\n",
      "Tuned sigma: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nprint(f\"Example removed from original dataset:\\n{model[removal_indices[0]]}\")\\nprint(f\"Does the example exist in the original dataset? - {np.any(np.all(initial_set == model[removal_indices[0]], axis=2))}\")\\nprint(f\"Does the example exist in the reduced dataset? - {np.any(np.all(padded_reduced_models == model[removal_indices[0]], axis=2))}\")\\n'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "fire_enn = enn(fire_data, 'regression', k_n=fire_knn.k_n, sigma=fire_knn.sigma)\n",
    "fire_enn.tune(demo=True)\n",
    "\n",
    "# Code for printing whether the removed example is in the dataset\n",
    "'''\n",
    "print(f\"Example removed from original dataset:\\n{model[removal_indices[0]]}\")\n",
    "print(f\"Does the example exist in the original dataset? - {np.any(np.all(initial_set == model[removal_indices[0]], axis=2))}\")\n",
    "print(f\"Does the example exist in the reduced dataset? - {np.any(np.all(padded_reduced_models == model[removal_indices[0]], axis=2))}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: [[0.6        0.45811966]\n",
      " [0.6        0.56746032]\n",
      " [0.73684211 0.56296296]\n",
      " [0.68421053 0.61050061]\n",
      " [0.57894737 0.53535354]\n",
      " [0.52631579 0.47125097]\n",
      " [0.57894737 0.38      ]\n",
      " [0.73684211 0.6167033 ]\n",
      " [0.78947368 0.72857143]\n",
      " [0.73684211 0.65836386]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "fire_enn_results = fire_enn.regress();\n",
    "glass_enn_results = glass_enn.classify();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Point being associated with a cluster for K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data point:\n",
      "[ 1.51 13.7   3.93  1.54 71.8   0.54  8.21  0.    0.15  2.  ]\n",
      "distances to centroids:\n",
      "[1.79824915 1.67263266 2.09234318 5.58875657 0.56284989 2.11520685\n",
      " 1.43425939 1.83226636 6.69772349 2.02380829 1.75988636 1.92010416\n",
      " 1.78468485 1.06094298 9.11172322 2.25386335 0.94270886 6.85979592\n",
      " 1.69251292 1.42734018 6.20328945 2.00227371 2.24822152 2.10653744\n",
      " 1.3065221  7.84554651 2.17726893 2.11397729 2.08124962 1.17162281\n",
      " 3.46151701 2.12287541 1.69655533 0.71533209 2.49747873 4.5846592\n",
      " 4.27773304 1.6522409  8.3587858 ]\n",
      "Data point [ 1.51 13.7   3.93  1.54 71.8   0.54  8.21  0.    0.15  2.  ] assigned to cluster 5\n",
      "\n",
      "\n",
      "Data point:\n",
      "[ 1.51 13.7   3.93  1.54 71.8   0.54  8.21  0.    0.15  2.  ]\n",
      "distances to centroids:\n",
      "[1.92618867 1.39988839 2.09234318 5.58875657 0.75361352 2.08305331\n",
      " 1.45313454 1.74477529 6.1995203  1.96329089 1.75988636 1.56846167\n",
      " 2.05648838 1.10193265 7.76578715 2.24330114 0.94129344 6.73209853\n",
      " 1.60491375 1.48873525 5.3031982  1.8939905  2.14172049 2.05327787\n",
      " 1.38310942 7.84554651 2.22208236 2.11397729 1.84731562 1.16534115\n",
      " 3.74748657 2.59704961 1.36652387 0.71249702 2.67872638 4.83124862\n",
      " 5.11449411 1.64986363 7.69087844]\n",
      "Data point [ 1.51 13.7   3.93  1.54 71.8   0.54  8.21  0.    0.15  2.  ] assigned to cluster 34\n",
      "\n",
      "\n",
      "Data point:\n",
      "[ 1.51 13.7   3.93  1.54 71.8   0.54  8.21  0.    0.15  2.  ]\n",
      "distances to centroids:\n",
      "[1.92618867 1.3818958  2.09234318 5.58875657 1.13042028 2.08305331\n",
      " 1.45313454 1.8645632  5.8555233  1.96329089 1.92612305 1.49876616\n",
      " 2.05648838 1.02091995 7.76578715 2.22046291 0.97047411 6.73209853\n",
      " 1.57672029 1.48873525 5.3031982  1.88517904 2.14172049 2.05327787\n",
      " 1.42651619 7.84554651 2.28973798 2.11397729 1.84731562 1.16534115\n",
      " 3.74748657 3.84825935 1.39749776 0.54764222 2.67872638 4.83124862\n",
      " 6.05579337 1.64986363 7.69087844]\n",
      "Data point [ 1.51 13.7   3.93  1.54 71.8   0.54  8.21  0.    0.15  2.  ] assigned to cluster 34\n",
      "\n",
      "\n",
      "Data point:\n",
      "[ 1.51 13.7   3.93  1.54 71.8   0.54  8.21  0.    0.15  2.  ]\n",
      "distances to centroids:\n",
      "[1.92618867 1.3818958  2.09234318 6.07565017 1.73787226 2.08305331\n",
      " 1.45313454 1.8645632  5.5892507  1.96329089 1.92612305 1.5308204\n",
      " 1.95767081 1.03253464 7.76578715 2.22046291 0.94837347 6.73209853\n",
      " 1.56099802 1.48873525 5.3031982  1.88517904 2.14172049 2.05327787\n",
      " 1.41270547 7.84554651 2.28973798 2.11397729 1.84731562 1.16534115\n",
      " 3.74748657 3.84825935 1.32021779 0.45239363 2.67872638 4.83124862\n",
      " 7.90500633 1.63372993 7.69087844]\n",
      "Data point [ 1.51 13.7   3.93  1.54 71.8   0.54  8.21  0.    0.15  2.  ] assigned to cluster 34\n",
      "\n",
      "\n",
      "Data point:\n",
      "[ 1.51 13.7   3.93  1.54 71.8   0.54  8.21  0.    0.15  2.  ]\n",
      "distances to centroids:\n",
      "[1.92618867 1.3818958  2.09234318 6.07565017 1.73787226 2.08305331\n",
      " 1.45313454 1.8645632  5.5892507  1.96329089 1.92612305 1.5308204\n",
      " 1.95767081 1.03253464 7.76578715 2.22046291 0.9174702  6.73209853\n",
      " 1.56099802 1.48873525 5.3031982  1.88517904 2.14172049 2.05327787\n",
      " 1.41270547 7.84554651 2.28973798 2.11397729 1.84731562 1.16534115\n",
      " 3.74748657 3.84825935 1.32021779 0.39643726 2.67872638 4.83124862\n",
      " 7.90500633 1.63372993 7.69087844]\n",
      "Data point [ 1.51 13.7   3.93  1.54 71.8   0.54  8.21  0.    0.15  2.  ] assigned to cluster 34\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: [[0.6        0.64803922]\n",
      " [0.6        0.4993895 ]\n",
      " [0.73684211 0.81565657]\n",
      " [0.68421053 0.5620915 ]\n",
      " [0.52631579 0.52136752]\n",
      " [0.63157895 0.55555556]\n",
      " [0.52631579 0.41558442]\n",
      " [0.63157895 0.59736264]\n",
      " [0.63157895 0.52161172]\n",
      " [0.68421053 0.53174603]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "glass_kmeans.classify(demo=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average classification performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glass Classification Performance:\n",
      "KNN     - 0/1 Loss: 0.6723684210526315 F1 Score: 0.5765411010558068\n",
      "ENN     - 0/1 Loss: 0.6568421052631579 F1 Score: 0.5589286639286639\n",
      "K-Means - 0/1 Loss: 0.6463157894736842 F1 Score: 0.5253235653235653\n"
     ]
    }
   ],
   "source": [
    "print(\"Glass Classification Performance:\")\n",
    "print(f\"KNN     - 0/1 Loss: {np.mean(glass_knn_results[:, 0])} F1 Score: {np.mean(glass_knn_results[:, 1])}\")\n",
    "print(f\"ENN     - 0/1 Loss: {np.mean(glass_enn_results[:, 0])} F1 Score: {np.mean(glass_enn_results[:, 1])}\")\n",
    "print(f\"K-Means - 0/1 Loss: {np.mean(glass_kmeans_results[:, 0])} F1 Score: {np.mean(glass_kmeans_results[:, 1])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average regression performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fire Regresssion Performance:\n",
      "KNN     - MSE: 0.0007571914245353266 MAE: 0.06275501301324626\n",
      "ENN     - MSE: 0.0015110256198391505 MAE: 0.03478500484367331\n",
      "K-Means - MSE: 0.00339409430679491 MAE: 0.04929409933155525\n"
     ]
    }
   ],
   "source": [
    "print(\"Fire Regresssion Performance:\")\n",
    "print(f\"KNN     - MSE: {np.mean(fire_knn_results[:, 0])} MAE: {np.mean(fire_knn_results[:, 1])}\")\n",
    "print(f\"ENN     - MSE: {np.mean(fire_enn_results[:, 0])} MAE: {np.mean(fire_enn_results[:, 1])}\")\n",
    "print(f\"K-Means - MSE: {np.mean(fire_kmeans_results[:, 0])} MAE: {np.mean(fire_kmeans_results[:, 1])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
